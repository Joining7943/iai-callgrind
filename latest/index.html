<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>README - Iai-Callgrind Documentation</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="Iai-Callgrind, a high-precision and consistent one-shot benchmarking framework/harness for Rust">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="css/dropdown.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="intro.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="index.html" class="active"><strong aria-hidden="true">2.</strong> README</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        <div class="dropdown">
                          <button onclick="versionDropdown()" class="icon-button version-dropdown-button" type="button">Version</button>
                          <div id="version-dropdown" class="version-dropdown-content">
                            <a href="../latest/index.html">Latest</a>
                            <hr class="sidbar-spacer">
                            <script src="../versions.js"></script>
                          </div>
                        </div>
                    </div>

                    <h1 class="menu-title">Iai-Callgrind Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/iai-callgrind/iai-callgrind" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                        <a href="https://github.com/iai-callgrind/iai-callgrind/master/docs/src/README.md" title="Suggest an edit" aria-label="Suggest an edit">
                            <i id="git-edit-button" class="fa fa-edit"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <!-- spell-checker: ignore fixt binstall libtest eprintln usize -->
<h1 align="center">Iai-Callgrind</h1>
<div align="center">High-precision and consistent benchmarking framework/harness for Rust</div>
<div align="center">
    <a href="https://docs.rs/crate/iai-callgrind/">Released API Docs</a>
    |
    <a href="https://github.com/iai-callgrind/iai-callgrind/blob/main/CHANGELOG.md">Changelog</a>
</div>
<br>
<div align="center">
    <a href="https://github.com/iai-callgrind/iai-callgrind/actions/workflows/cicd.yml">
        <img src="https://github.com/iai-callgrind/iai-callgrind/actions/workflows/cicd.yml/badge.svg" alt="GitHub branch checks state"/>
    </a>
    <a href="https://crates.io/crates/iai-callgrind">
        <img src="https://img.shields.io/crates/v/iai-callgrind.svg" alt="Crates.io"/>
    </a>
    <a href="https://docs.rs/iai-callgrind/">
        <img src="https://docs.rs/iai-callgrind/badge.svg" alt="docs.rs"/>
    </a>
    <a href="https://github.com/rust-lang/rust">
        <img src="https://img.shields.io/badge/MSRV-1.66.0-brightgreen" alt="MSRV"/>
    </a>
</div>
<p>Iai-Callgrind is a benchmarking framework/harness which primarily uses
<a href="https://valgrind.org/docs/manual/cl-manual.html">Valgrind's Callgrind</a> and the
other Valgrind tools to provide extremely accurate and consistent measurements
of Rust code, making it perfectly suited to run in environments like a CI.</p>
<p>This crate started as a fork of the great <a href="https://github.com/bheisler/iai">Iai</a> crate rewritten to
use Valgrind's <a href="https://valgrind.org/docs/manual/cl-manual.html">Callgrind</a> instead of
<a href="https://valgrind.org/docs/manual/cg-manual.html">Cachegrind</a> but also adds a lot of other
improvements and features.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of Contents</a></h2>
<ul>
<li><a href="#table-of-contents">Table of Contents</a>
<ul>
<li><a href="#features">Features</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#benchmarking">Benchmarking</a>
<ul>
<li><a href="#library-benchmarks">Library Benchmarks</a></li>
<li><a href="#binary-benchmarks">Binary Benchmarks</a></li>
</ul>
</li>
<li><a href="#performance-regressions">Performance Regressions</a></li>
<li><a href="#valgrind-tools">Valgrind Tools</a></li>
<li><a href="#valgrind-client-requests">Valgrind Client Requests</a></li>
<li><a href="#flamegraphs">Flamegraphs</a></li>
<li><a href="#command-line-arguments-and-environment-variables">Command-line arguments and environment variables</a>
<ul>
<li><a href="#comparing-with-baselines">Baselines</a></li>
<li><a href="#customize-the-output-directory">Output directory</a></li>
<li><a href="#machine-readable-output">Machine-readable output</a></li>
<li><a href="#other-output-options">Other output options</a></li>
</ul>
</li>
<li><a href="#features-and-differences-to-iai">Features and differences to Iai</a></li>
<li><a href="#faq">FAQ</a></li>
<li><a href="#what-hasnt-changed">What hasn't changed</a></li>
<li><a href="#see-also">See also</a></li>
<li><a href="#contributing">Contributing</a></li>
<li><a href="#credits">Credits</a></li>
<li><a href="#license">License</a></li>
</ul>
</li>
</ul>
<h3 id="features"><a class="header" href="#features">Features</a></h3>
<ul>
<li><strong>Precision</strong>: High-precision measurements allow you to reliably detect very
small optimizations of your code</li>
<li><strong>Consistency</strong>: Iai-Callgrind can take accurate measurements even in
virtualized CI environments</li>
<li><strong>Performance</strong>: Since Iai-Callgrind only executes a benchmark once, it is
typically a lot faster to run than benchmarks measuring the execution and wall
time</li>
<li><strong>Regression</strong>: Iai-Callgrind reports the difference between benchmark runs to
make it easy to spot detailed performance regressions and improvements. You
can define limits for specific event kinds to fail a benchmark if that limit
is breached.</li>
<li><strong>CPU and Cache Profiling</strong>: Iai-Callgrind generates a Callgrind profile of
your code while benchmarking, so you can use Callgrind-compatible tools like
<a href="https://valgrind.org/docs/manual/cl-manual.html#cl-manual.callgrind_annotate-options">callgrind_annotate</a>
or the visualizer <a href="https://kcachegrind.github.io/html/Home.html">kcachegrind</a>
to analyze the results in detail.</li>
<li><strong>Memory Profiling</strong>: You can run other Valgrind tools like <a href="https://valgrind.org/docs/manual/dh-manual.html">DHAT: a dynamic
heap analysis tool</a> and
<a href="https://valgrind.org/docs/manual/ms-manual.html">Massif: a heap profiler</a>
with the Iai-Callgrind benchmarking framework. Their profiles are stored next
to the callgrind profiles and are ready to be examined with analyzing tools
like <code>dh_view.html</code>, <code>ms_print</code> and others.</li>
<li><strong>Visualization</strong>: Iai-Callgrind is capable of creating regular and
differential flamegraphs from the Callgrind output format.</li>
<li><strong>Valgrind Client Requests</strong>: Support of zero overhead <a href="https://valgrind.org/docs/manual/manual-core-adv.html#manual-core-adv.clientreq">Valgrind Client
Requests</a>
(compared to native valgrind client requests overhead) on many targets</li>
<li><strong>Stable-compatible</strong>: Benchmark your code without installing nightly Rust</li>
</ul>
<h3 id="installation"><a class="header" href="#installation">Installation</a></h3>
<p>In order to use Iai-Callgrind, you must have <a href="https://www.valgrind.org">Valgrind</a> installed. This
means that Iai-Callgrind cannot be used on platforms that are not supported by Valgrind.</p>
<p>To start with Iai-Callgrind, add the following to your <code>Cargo.toml</code> file:</p>
<pre><code class="language-toml">[dev-dependencies]
iai-callgrind = "0.12.3"
</code></pre>
<p>To be able to run the benchmarks you'll also need the <code>iai-callgrind-runner</code> binary installed
somewhere in your <code>$PATH</code>, for example with</p>
<pre><code class="language-shell">cargo install --version 0.12.3 iai-callgrind-runner
</code></pre>
<p>or with <code>binstall</code></p>
<pre><code class="language-shell">cargo binstall iai-callgrind-runner@0.12.3
</code></pre>
<p>There's also the possibility to install the binary somewhere else and point the
<code>IAI_CALLGRIND_RUNNER</code> environment variable to the absolute path of the <code>iai-callgrind-runner</code>
binary like so:</p>
<pre><code class="language-shell">cargo install --version 0.12.3 --root /tmp iai-callgrind-runner
IAI_CALLGRIND_RUNNER=/tmp/bin/iai-callgrind-runner cargo bench --bench my-bench
</code></pre>
<p>When updating the <code>iai-callgrind</code> library, you'll also need to update
<code>iai-callgrind-runner</code> and vice-versa or else the benchmark runner will exit
with an error. Otherwise, there is no need to interact with
<code>iai-callgrind-runner</code> as it is just an implementation detail.</p>
<p>Since the <code>iai-callgrind-runner</code> version must match the <code>iai-callgrind</code> library
version it's best to automate this step in the CI. A job step in the github
actions ci could look like this</p>
<pre><code class="language-yaml">- name: Install iai-callgrind-runner
  run: |
    version=$(cargo metadata --format-version=1 |\
      jq '.packages[] | select(.name == "iai-callgrind").version' |\
      tr -d '"'
    )
    cargo install iai-callgrind-runner --version $version
</code></pre>
<p>If you want to make use of the <a href="#valgrind-client-requests">Valgrind Client
Requests</a> you need <code>libclang</code> (clang &gt;= 5.0)
installed. See also the requirements of
<a href="https://rust-lang.github.io/rust-bindgen/requirements.html">bindgen</a>) and of
<a href="https://github.com/rust-lang/cc-rs">cc</a>.</p>
<p><code>iai-callgrind</code> needs the debug symbols when running the benchmarks. There are
multiple places where you can configure profiles, see the
<a href="#benchmarking">Benchmarking</a> section below for more details.</p>
<h3 id="benchmarking"><a class="header" href="#benchmarking">Benchmarking</a></h3>
<p><code>iai-callgrind</code> can be used to benchmark libraries or binaries. Library benchmarks benchmark
functions and methods of a crate and binary benchmarks benchmark the executables of a crate. The
different benchmark types cannot be intermixed in the same benchmark file but having different
benchmark files for library and binary benchmarks is no problem. More on that in the following
sections.</p>
<p>For a quickstart and examples of benchmarking libraries see the <a href="#library-benchmarks">Library Benchmark
Section</a> and for executables see the <a href="#binary-benchmarks">Binary Benchmark
Section</a>. Read the <a href="https://docs.rs/iai-callgrind/0.12.3/iai_callgrind/">docs</a>!</p>
<p>As mentioned in above in the <code>Installation</code> section, it's required to run the
benchmarks with debugging symbols switched on. For example in your
<code>~/.cargo/config</code> or your project's <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[profile.bench]
debug = true
</code></pre>
<p>Now, all benchmarks you run with <code>cargo bench</code> include the debug symbols. (See also
<a href="https://doc.rust-lang.org/cargo/reference/profiles.html">Cargo Profiles</a> and
<a href="https://doc.rust-lang.org/cargo/reference/config.html">Cargo Config</a>).</p>
<p>It's required that settings like <code>strip = true</code> or other configuration options
stripping the debug symbols need to be disabled explicitly for the <code>bench</code>
profile if you have changed this option for the <code>release</code> profile. For example:</p>
<pre><code class="language-toml">[profile.release]
strip = true

[profile.bench]
debug = true
strip = false
</code></pre>
<p>Per default, <code>iai-callgrind</code> runs all benchmarks with Valgrind's cache
simulation turned on (<code>--cache-sim=yes</code>) in order to calculate an estimation for
the total cpu cycles. See also the <a href="#rework-the-metrics-output">Metrics Output</a>
section for more infos. However, if you want to opt-out of the cache simulation
and the calculation of estimated cycles, you can easily do so within the
benchmark with the <code>LibraryBenchmarkConfig</code> (or <code>BinaryBenchmarkConfig</code>):</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>LibraryBenchmarkConfig::default().raw_callgrind_args(["--cache-sim=no"])
<span class="boring">}</span></code></pre></pre>
<p>in the cli with environment variables</p>
<pre><code class="language-shell">IAI_CALLGRIND_CALLGRIND_ARGS="--cache-sim=no" cargo bench
</code></pre>
<p>or with arguments</p>
<pre><code class="language-shell">cargo bench -- --callgrind-args="--cache-sim=no"
</code></pre>
<p>To be able to run the latter command, some <a href="#running-cargo-bench-results-in-an-unrecognized-option-error">additional
configuration</a>
might be needed.</p>
<h4 id="library-benchmarks"><a class="header" href="#library-benchmarks">Library Benchmarks</a></h4>
<p>Use this scheme if you want to micro-benchmark specific functions of your crate's library.</p>
<h5 id="important-default-behavior"><a class="header" href="#important-default-behavior">Important default behavior</a></h5>
<p>The environment variables are cleared before running a library benchmark. Have a
look into the <a href="#configuration">Configuration</a> section if you need to change that
behavior.</p>
<h5 id="quickstart"><a class="header" href="#quickstart">Quickstart</a></h5>
<p>Add</p>
<pre><code class="language-toml">[[bench]]
name = "my_benchmark"
harness = false
</code></pre>
<p>to your <code>Cargo.toml</code> file and then create a file with the same <code>name</code> in <code>benches/my_benchmark.rs</code>
with the following content:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use iai_callgrind::{main, library_benchmark_group, library_benchmark};
use std::hint::black_box;

fn fibonacci(n: u64) -&gt; u64 {
    match n {
        0 =&gt; 1,
        1 =&gt; 1,
        n =&gt; fibonacci(n - 1) + fibonacci(n - 2),
    }
}

#[library_benchmark]
#[bench::short(10)]
#[bench::long(30)]
fn bench_fibonacci(value: u64) -&gt; u64 {
    black_box(fibonacci(value))
}

library_benchmark_group!(
    name = bench_fibonacci_group;
    benchmarks = bench_fibonacci
);

main!(library_benchmark_groups = bench_fibonacci_group);
<span class="boring">}</span></code></pre></pre>
<p>Note that it is important to annotate the benchmark functions with <code>#[library_benchmark]</code>. But,
there's no need to annotate benchmark functions with <code>inline(never)</code> anymore. The <code>bench</code> attribute
takes any expression what includes function calls. The following would have worked too and avoids
setup code within the benchmark function eliminating the need to pass <code>toggle-collect</code> arguments to
callgrind:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn some_setup_func(value: u64) -&gt; u64 {
    value
}

#[library_benchmark]
#[bench::long(some_setup_func(30))]
fn bench_fibonacci(value: u64) -&gt; u64 {
    black_box(fibonacci(value))
}
<span class="boring">}</span></code></pre></pre>
<p>Now, you can run this benchmark with <code>cargo bench --bench my_benchmark</code> in your project root and you
should see something like this:</p>
<pre><code class="language-text">test_lib_bench_readme_example_fibonacci::bench_fibonacci_group::bench_fibonacci short:10
  Instructions:                1733|N/A             (*********)
  L1 Hits:                     2359|N/A             (*********)
  L2 Hits:                        0|N/A             (*********)
  RAM Hits:                       2|N/A             (*********)
  Total read+write:            2361|N/A             (*********)
  Estimated Cycles:            2429|N/A             (*********)
test_lib_bench_readme_example_fibonacci::bench_fibonacci_group::bench_fibonacci long:30
  Instructions:            26214733|N/A             (*********)
  L1 Hits:                 35638617|N/A             (*********)
  L2 Hits:                        0|N/A             (*********)
  RAM Hits:                       4|N/A             (*********)
  Total read+write:        35638621|N/A             (*********)
  Estimated Cycles:        35638757|N/A             (*********)
</code></pre>
<p>In addition, you'll find the callgrind output in <code>target/iai</code>, if you want to investigate further
with a tool like <code>callgrind_annotate</code>. When running the same benchmark again, the output will
report the differences between the current and the previous run. Say you've made change to the
<code>fibonacci</code> function, then you may see something like this:</p>
<pre><code class="language-text">test_lib_bench_readme_example_fibonacci::bench_fibonacci_group::bench_fibonacci short:10
  Instructions:                2804|1733            (+61.8003%) [+1.61800x]
  L1 Hits:                     3815|2359            (+61.7211%) [+1.61721x]
  L2 Hits:                        0|0               (No change)
  RAM Hits:                       2|2               (No change)
  Total read+write:            3817|2361            (+61.6688%) [+1.61669x]
  Estimated Cycles:            3885|2429            (+59.9424%) [+1.59942x]
test_lib_bench_readme_example_fibonacci::bench_fibonacci_group::bench_fibonacci long:30
  Instructions:            16201596|26214733        (-38.1966%) [-1.61803x]
  L1 Hits:                 22025878|35638617        (-38.1966%) [-1.61803x]
  L2 Hits:                        0|0               (No change)
  RAM Hits:                       3|4               (-25.0000%) [-1.33333x]
  Total read+write:        22025881|35638621        (-38.1966%) [-1.61803x]
  Estimated Cycles:        22025983|35638757        (-38.1965%) [-1.61803x]
</code></pre>
<h5 id="the-library_benchmark-attribute-in-more-detail"><a class="header" href="#the-library_benchmark-attribute-in-more-detail">The #[library_benchmark] attribute in more detail</a></h5>
<p>This attribute needs to be present on all benchmark functions specified in the
<code>library_benchmark_group</code>. The benchmark function can then be further annotated
with the <code>#[bench]</code> or <code>#[benches]</code> attributes.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[library_benchmark]
#[bench::first(21)]
fn my_bench(value: u64) -&gt; u64 {
    // benchmark something
}

library_benchmark_group!(name = my_group; benchmarks = my_bench);
<span class="boring">}</span></code></pre></pre>
<p>The following parameters are accepted:</p>
<ul>
<li><code>config</code>: Accepts a <code>LibraryBenchmarkConfig</code></li>
<li><code>setup</code>: A global setup function which is applied to all following <code>#[bench]</code>
and <code>#[benches]</code> attributes if not overwritten by a <code>setup</code> parameter of these
attributes.</li>
<li><code>teardown</code>: Similar to <code>setup</code> but takes a global <code>teardown</code> function.</li>
</ul>
<p>A short example on the usage of the <code>setup</code> parameter:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn my_setup(value: u64) -&gt; String {
     format!("{value}")
}

fn my_other_setup(value: u64) -&gt; String {
     format!("{}", value + 10)
}

#[library_benchmark(setup = my_setup)]
#[bench::first(21)]
#[benches::multiple(42, 84)]
#[bench::last(args = (102), setup = my_other_setup)]
fn my_bench(value: String) {
    println!("{value}");
}
<span class="boring">}</span></code></pre></pre>
<p>Here, the benchmarks with the id <code>first</code> and <code>multiple</code> use the <code>my_setup</code>
function, and <code>last</code> uses <code>my_other_setup</code>.</p>
<p>And a short example of the <code>teardown</code> parameter:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn my_teardown(value: usize) {
     println!("The length of the input string was: {value}");
}

fn my_other_teardown(value: usize) {
     if value != 3 {
         panic!("The length of the input string was: {value} but expected it to be 3");
     }
}

#[library_benchmark(teardown = my_teardown)]
#[bench::first("1")]
#[benches::multiple("42", "84")]
#[bench::last(args = ("104"), teardown = my_other_teardown)]
fn my_bench(value: &amp;str) -&gt; usize {
    // Let's benchmark the `len` function
    black_box(value.len())
}
<span class="boring">}</span></code></pre></pre>
<p>This example works well with the <code>--nocapture</code> option (env: <code>IAI_CALLGRIND_NOCAPTURE</code>,
see also <a href="#show-terminal-output-of-benchmarks">Show terminal output of
benchmarks</a>), so you can actually see
the output of the <code>my_teardown</code> function.</p>
<h5 id="the-bench-attribute"><a class="header" href="#the-bench-attribute">The #[bench] attribute</a></h5>
<p>The basic structure is <code>#[bench::some_id(/* parameters */)]</code>. The part after the
<code>::</code> must be an id unique within the same <code>#[library_benchmark]</code>. This attribute
accepts the following parameters:</p>
<ul>
<li><code>args</code>: A tuple with a list of arguments which are passed to the
benchmark function. The parentheses also need to be present if there is only a
single argument (<code>#[bench::my_id(args = (10))]</code>).</li>
<li><code>config</code>: Accepts a <code>LibraryBenchmarkConfig</code></li>
<li><code>setup</code>: A function which takes the arguments specified in the <code>args</code>
parameter and passes its return value to the benchmark function.</li>
<li><code>teardown</code>: A function which takes the return value of the benchmark function.</li>
</ul>
<p>If no other parameters besides <code>args</code> are present you can simply pass the
arguments as a list of values. Instead of <code>#[bench::my_id(args = (10, 20))]</code>,
you could also use the shorter <code>#[bench::my_id(10, 20)]</code>.</p>
<h5 id="specify-multiple-benchmarks-at-once-with-the-benches-attribute"><a class="header" href="#specify-multiple-benchmarks-at-once-with-the-benches-attribute">Specify multiple benchmarks at once with the #[benches] attribute</a></h5>
<p>This attribute accepts the same parameters as the <code>#[bench]</code> attribute: <code>args</code>,
<code>config</code>, <code>setup</code> and <code>teardown</code> and additionally the <code>file</code> parameter. In
contrast to the <code>args</code> parameter in <code>#[bench]</code>, <code>args</code> takes an array of
arguments.</p>
<p>Let's start with an example:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>fn setup_worst_case_array(start: i32) -&gt; Vec&lt;i32&gt; {
    if start.is_negative() {
        (start..0).rev().collect()
    } else {
        (0..start).rev().collect()
    }
}

#[library_benchmark]
#[benches::multiple(vec![1], vec![5])]
#[benches::with_setup(args = [1, 5], setup = setup_worst_case_array)]
fn bench_bubble_sort_with_benches_attribute(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(bubble_sort(input))
}
<span class="boring">}</span></code></pre></pre>
<p>Usually the <code>arguments</code> are passed directly to the benchmarking function as it
can be seen in the <code>#[benches::multiple(...)]</code> case. In
<code>#[benches::with_setup(...)]</code>, the arguments are passed to the <code>setup</code> function
instead. The above <code>#[library_benchmark]</code> is pretty much the same as</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[library_benchmark]
#[bench::multiple_0(vec![1])]
#[bench::multiple_1(vec![5])]
#[bench::with_setup_0(setup_worst_case_array(1)])]
#[bench::with_setup_1(setup_worst_case_array(5)])]
fn bench_bubble_sort_with_benches_attribute(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(bubble_sort(input))
}
<span class="boring">}</span></code></pre></pre>
<p>but a lot more concise especially if a lot of values are passed to the same
<code>setup</code> function.</p>
<p>The <code>file</code> parameter goes a step further and reads the specified file line by
line creating a benchmark from each line. The line is passed to the benchmark
function as <code>String</code> or if the <code>setup</code> parameter is also present to the <code>setup</code>
function. A small example assuming you have a file <code>benches/inputs</code> (relative
paths are interpreted to the workspace root) with the following content</p>
<pre><code class="language-text">1
11
111
</code></pre>
<p>then</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[library_benchmark]
#[benches::by_file(file = "benches/inputs")]
fn some_bench(line: String) -&gt; Result&lt;u64&gt; {
    black_box(my_lib::string_to_u64(line))
}
<span class="boring">}</span></code></pre></pre>
<p>The above is roughly equivalent to the following but with the <code>args</code> parameter</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[library_benchmark]
#[benches::by_file(args = [1.to_string(), 11.to_string(), 111.to_string()])]
fn some_bench(line: String) -&gt; Result&lt;u64&gt; {
    black_box(my_lib::string_to_u64(line))
}
<span class="boring">}</span></code></pre></pre>
<p>Reading inputs from a file allows for example sharing the same inputs between
different benchmarking frameworks like <code>criterion</code> or if you simply have a long
list of inputs you might find it more convenient to read them from a file.</p>
<h5 id="the-library_benchmark_group"><a class="header" href="#the-library_benchmark_group">The <code>library_benchmark_group!</code></a></h5>
<p>The <code>library_benchmark_group</code> macro accepts the following parameters (in this
order and separated by a semicolon):</p>
<ul>
<li><strong><code>name</code></strong> (mandatory): A unique name used to identify the group for the
<code>main!</code> macro</li>
<li><strong><code>config</code></strong> (optional): A <code>LibraryBenchmarkConfig</code> which is applied
to all benchmarks within the same group.</li>
<li><strong><code>compare_by_id</code></strong> (optional): The default is false. If true, all benches in
the benchmark functions specified with the <code>benchmarks</code> argument, across any
benchmark groups, are compared with each other as long as the ids (the part
after the <code>::</code> in <code>#[bench::id(...)]</code>) match. See also
<a href="#comparing-benchmark-functions">below</a></li>
<li><strong><code>setup</code></strong> (optional): A setup function or any valid expression which is run
before all benchmarks of this group</li>
<li><strong><code>teardown</code></strong> (optional): A teardown function or any valid expression which
is run after all benchmarks of this group</li>
<li><strong><code>benchmarks</code></strong> (mandatory): A list of comma separated paths of benchmark
functions which are annotated with <code>#[library_benchmark]</code></li>
</ul>
<p>Note the <code>setup</code> and <code>teardown</code> parameters are different to the ones of
<code>#[library_benchmark]</code>, <code>#[bench]</code> and <code>#[benches]</code>. They accept an expression
or function call as in <code>setup = group_setup_function()</code>. Also, these <code>setup</code> and
<code>teardown</code> functions are not overridden by the ones from any of the before
mentioned attributes.</p>
<h6 id="comparing-benchmark-functions"><a class="header" href="#comparing-benchmark-functions">Comparing benchmark functions</a></h6>
<p>Comparing benchmark functions is supported via the optional
<code>library_benchmark_group!</code> argument <code>compare_by_id</code> (The default value for
<code>compare_by_id</code> is <code>false</code>). Only benches with the same <code>id</code> are compared, which
allows to single out cases which don't need to be compared. In the following
example, the <code>case_3</code> and <code>multiple</code> bench are compared with each other in
addition to the usual comparison with the previous run:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[library_benchmark]
#[bench::case_3(vec![1, 2, 3])]
#[benches::multiple(args = [vec![1, 2], vec![1, 2, 3, 4]])]
fn bench_bubble_sort_best_case(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(bubble_sort(input))
}

#[library_benchmark]
#[bench::case_3(vec![3, 2, 1])]
#[benches::multiple(args = [vec![2, 1], vec![4, 3, 2, 1]])]
fn bench_bubble_sort_worst_case(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(bubble_sort(input))
}

library_benchmark_group!(
    name = bench_bubble_sort;
    compare_by_id = true;
    benchmarks = bench_bubble_sort_best_case, bench_bubble_sort_worst_case
);
<span class="boring">}</span></code></pre></pre>
<p>Note if <code>compare_by_id</code> is <code>true</code>, all benchmark functions are compared with
each other, so you are not limited to two benchmark functions per comparison
group.</p>
<p>Here's a curated excerpt from the output of the above example to see what is
happening:</p>
<pre><code class="language-text">test_lib_bench_compare::bubble_sort_compare::bench_bubble_sort_best_case case_3:vec! [1, 2, 3]
  Instructions:                  94|N/A             (*********)
  L1 Hits:                      124|N/A             (*********)
  L2 Hits:                        0|N/A             (*********)
  RAM Hits:                       4|N/A             (*********)
  Total read+write:             128|N/A             (*********)
  Estimated Cycles:             264|N/A             (*********)
test_lib_bench_compare::bubble_sort_compare::bench_bubble_sort_worst_case case_3:vec! [3, 2, 1]
  Instructions:                 103|N/A             (*********)
  L1 Hits:                      138|N/A             (*********)
  L2 Hits:                        0|N/A             (*********)
  RAM Hits:                       5|N/A             (*********)
  Total read+write:             143|N/A             (*********)
  Estimated Cycles:             313|N/A             (*********)
  Comparison with bench_bubble_sort_best_case case_3:vec! [1, 2, 3]
  Instructions:                  94|103             (-8.73786%) [-1.09574x]
  L1 Hits:                      124|138             (-10.1449%) [-1.11290x]
  L2 Hits:                        0|0               (No change)
  RAM Hits:                       4|5               (-20.0000%) [-1.25000x]
  Total read+write:             128|143             (-10.4895%) [-1.11719x]
  Estimated Cycles:             264|313             (-15.6550%) [-1.18561x]
</code></pre>
<p>Here's the procedure of the comparison algorithm:</p>
<ol>
<li>Run all benches in the first benchmark function</li>
<li>Run the first bench in the second benchmark function and if there is a bench
in the first benchmark function with the same id compare them</li>
<li>Run the second bench in the second benchmark function ...</li>
<li>...</li>
<li>Run the first bench in the third benchmark function and if there is a bench
in the first benchmark function with the same id compare them. If there is a
bench with the same id in the second benchmark function compare them.</li>
<li>Run the second bench in the third benchmark function ...</li>
<li>and so on ... until all benches are compared with each other</li>
</ol>
<p>Neither the order nor the amount of benches within the benchmark functions
matters, so it is not strictly necessary to mirror the bench ids of the first
benchmark function in the second, third, etc. benchmark function.</p>
<h5 id="examples"><a class="header" href="#examples">Examples</a></h5>
<p>For a fully documented and working benchmark see the
<a href="benchmark-tests/benches/test_lib_bench_groups.rs">test_lib_bench_groups</a> benchmark file and read
the <a href="https://docs.rs/iai-callgrind/0.12.3/iai_callgrind/"><code>library documentation</code></a>!</p>
<h5 id="configuration"><a class="header" href="#configuration">Configuration</a></h5>
<p>It's possible to configure some of the behavior of <code>iai-callgrind</code>. See the <a href="https://docs.rs/iai-callgrind/0.12.3/iai_callgrind/">docs</a> of
<code>LibraryBenchmarkConfig</code> for more details. At top-level with the <code>main!</code> macro:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>main!(
    config = LibraryBenchmarkConfig::default();
    library_benchmark_groups = ...
);
<span class="boring">}</span></code></pre></pre>
<p>At group-level:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>library_benchmark_groups!(
    name = some_name;
    config = LibraryBenchmarkConfig::default();
    benchmarks = ...
);
<span class="boring">}</span></code></pre></pre>
<p>At <code>library_benchmark</code> level:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[library_benchmark(config = LibraryBenchmarkConfig::default())]
...
<span class="boring">}</span></code></pre></pre>
<p>and at <code>bench</code> level:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>#[library_benchmark]
#[bench::some_id(args = (1, 2), config = LibraryBenchmarkConfig::default()]
...
<span class="boring">}</span></code></pre></pre>
<p>The config at <code>bench</code> level overwrites the config at <code>library_benchmark</code> level. The config at
<code>library_benchmark</code> level overwrites the config at group level and so on. Note that configuration
values like <code>envs</code> are additive and don't overwrite configuration values of higher levels.</p>
<h3 id="binary-benchmarks"><a class="header" href="#binary-benchmarks">Binary Benchmarks</a></h3>
<p>Use this scheme to benchmark one or more binaries of your crate. If you really like to, it's
possible to benchmark any executable file in the <code>PATH</code> or any executable specified with an absolute
path.</p>
<p>It's also possible to run functions of the same benchmark file <code>before</code> and <code>after</code> all benchmarks
or to <code>setup</code> and <code>teardown</code> any benchmarked binary.</p>
<p>Unlike <a href="#library-benchmarks">Library Benchmarks</a>, there are no setup costs for binary benchmarks to
pay attention at, since each benchmark run's command is passed directly to valgrind's callgrind.</p>
<h4 id="temporary-workspace-and-other-important-default-behavior"><a class="header" href="#temporary-workspace-and-other-important-default-behavior">Temporary Workspace and other important default behavior</a></h4>
<p>Per default, all binary benchmarks and the <code>before</code>, <code>after</code>, <code>setup</code> and <code>teardown</code> functions are
executed in a temporary directory. See the <a href="#switching-off-the-sandbox">Switching off the sandbox</a>
for changing this behavior.</p>
<p>Also, the environment variables of benchmarked binaries are cleared before the benchmark is run. See
also <a href="#environment-variables">Environment variables</a> for how to pass environment variables to the
benchmarked binary.</p>
<h4 id="quickstart-1"><a class="header" href="#quickstart-1">Quickstart</a></h4>
<p>Suppose your crate's binary is named <code>benchmark-tests-printargs</code> and you have a
fixtures directory in <code>fixtures</code> with a file <code>test1.txt</code> in it:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use iai_callgrind::{
    binary_benchmark_group, main, Arg, BinaryBenchmarkConfig, BinaryBenchmarkGroup,
    Fixtures, Run,
};

fn my_setup() {
    println!("We can put code in here which will be run before each benchmark run");
}

// We specify a cmd `"benchmark-tests-exe"` for the whole group which is a
// binary of our crate. This eliminates the need to specify a `cmd` for each
// `Run` later on and we can use the auto-discovery of a crate's binary at group
// level. We'll also use the `setup` argument to run a function before each of
// the benchmark runs.
binary_benchmark_group!(
    name = my_exe_group;
    setup = my_setup;
    // This directory will be copied into the root of the sandbox (as `fixtures`)
    config = BinaryBenchmarkConfig::default().fixtures(Fixtures::new("fixtures"));
    benchmark =
        |"benchmark-tests-printargs", group: &amp;mut BinaryBenchmarkGroup| {
            setup_my_exe_group(group)
    }
);

// Working within a macro can be tedious sometimes so we moved the setup code
// into this method
fn setup_my_exe_group(group: &amp;mut BinaryBenchmarkGroup) {
    group
        // Setup our first run doing something with our fixture `test1.txt`. The
        // id (here `do foo with test1`) of an `Arg` has to be unique within the
        // same group
        .bench(Run::with_arg(Arg::new(
            "do foo with test1",
            ["--foo=fixtures/test1.txt"],
        )))

        // Setup our second run with two positional arguments. We're not
        // interested in anything happening before the main function in
        // `benchmark-tests-printargs`, so we set the entry_point.
        .bench(
            Run::with_arg(
                Arg::new(
                    "positional arguments",
                    ["foo", "foo bar"],
                )
            ).entry_point("benchmark_tests_printargs::main")
        )

        // Our last run doesn't take an argument at all.
        .bench(Run::with_arg(Arg::empty("no argument")));
}

// As last step specify all groups we want to benchmark in the main! macro
// argument `binary_benchmark_groups`. The main macro is always needed and
// finally expands to a benchmarking harness
main!(binary_benchmark_groups = my_exe_group);
<span class="boring">}</span></code></pre></pre>
<p>You're ready to run the benchmark with <code>cargo bench --bench my_binary_benchmark</code>.</p>
<p>The output of this benchmark run could look like this:</p>
<pre><code class="language-text">my_binary_benchmark::my_exe_group do foo with test1:benchmark-tests-printargs "--foo=fixt...
  Instructions:              331082|N/A             (*********)
  L1 Hits:                   442452|N/A             (*********)
  L2 Hits:                      720|N/A             (*********)
  RAM Hits:                    3926|N/A             (*********)
  Total read+write:          447098|N/A             (*********)
  Estimated Cycles:          583462|N/A             (*********)
my_binary_benchmark::my_exe_group positional arguments:benchmark-tests-printargs foo "foo ba...
  Instructions:                3906|N/A             (*********)
  L1 Hits:                     5404|N/A             (*********)
  L2 Hits:                        8|N/A             (*********)
  RAM Hits:                      91|N/A             (*********)
  Total read+write:            5503|N/A             (*********)
  Estimated Cycles:            8629|N/A             (*********)
my_binary_benchmark::my_exe_group no argument:benchmark-tests-printargs
  Instructions:              330070|N/A             (*********)
  L1 Hits:                   441031|N/A             (*********)
  L2 Hits:                      716|N/A             (*********)
  RAM Hits:                    3925|N/A             (*********)
  Total read+write:          445672|N/A             (*********)
  Estimated Cycles:          581986|N/A             (*********)
</code></pre>
<p>You'll find the callgrind output files of each run of the benchmark
<code>my_binary_benchmark</code> of the group <code>my_exe_group</code> in
<code>target/iai/$CARGO_PKG_NAME/my_binary_benchmark/my_exe_group</code>. See also
<a href="#customize-the-output-directory">customizing the output directory</a>.</p>
<h4 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h4>
<p>Much like the configuration of <a href="#configuration">Library Benchmarks</a> it's possible to configure
binary benchmarks at top-level in the <code>main!</code> macro and at group-level in the
<code>binary_benchmark_groups!</code> with the <code>config = ...;</code> argument. In contrast to library benchmarks,
binary benchmarks can be configured at a lower and last level within <code>Run</code> directly.</p>
<h4 id="auto-discovery-of-a-crates-binaries"><a class="header" href="#auto-discovery-of-a-crates-binaries">Auto-discovery of a crate's binaries</a></h4>
<p>Auto-discovery of a crate's binary works only when specifying the name of it at group level.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>binary_benchmark_group!(
    name = my_exe_group;
    benchmark = |"my-exe", group: &amp;mut BinaryBenchmarkGroup| {});
<span class="boring">}</span></code></pre></pre>
<p>If you don't like specifying a default command at group level, you can use
<code>env!("CARGO_BIN_EXE_name)</code> at <code>Run</code>-level like so:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>binary_benchmark_group!(
    name = my_exe_group;
    benchmark = |group: &amp;mut BinaryBenchmarkGroup| {
        group.bench(Run::with_cmd(env!("CARGO_BIN_EXE_my-exe"), Arg::empty("some id")));
    });
<span class="boring">}</span></code></pre></pre>
<h4 id="a-benchmark-run-of-a-binary-exits-with-error"><a class="header" href="#a-benchmark-run-of-a-binary-exits-with-error">A benchmark run of a binary exits with error</a></h4>
<p>Usually, if a benchmark exits with a non-zero exit code, the whole benchmark run fails and stops.
If you expect the exit code of your benchmarked binary to be different from <code>0</code>, you can set the
expected exit code with <code>Options</code> at <code>Run</code>-level</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>binary_benchmark_group!(
    name = my_exe_group;
    benchmark = |"my-exe", group: &amp;mut BinaryBenchmarkGroup| {
        group.bench(
            Run::with_arg(
                Arg::empty("some id")
            )
            .options(Options::default().exit_with(ExitWith::Code(100)))
        );
    });
<span class="boring">}</span></code></pre></pre>
<h4 id="environment-variables"><a class="header" href="#environment-variables">Environment variables</a></h4>
<p>Per default, the environment variables are cleared before running a benchmark.</p>
<p>It's possible to specify environment variables at <code>Run</code>-level which should be available in the
binary:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>binary_benchmark_group!(
    name = my_exe_group;
    benchmark = |"my-exe", group: &amp;mut BinaryBenchmarkGroup| {
        group.bench(Run::with_arg(Arg::empty("some id")).envs(["KEY=VALUE", "KEY"]));
    });
<span class="boring">}</span></code></pre></pre>
<p>Environment variables specified in the <code>envs</code> array are usually <code>KEY=VALUE</code> pairs. But, if
<code>env_clear</code> is true (what is the default), single <code>KEY</code>s are environment variables to pass-through
to the <code>cmd</code>. Pass-through environment variables are ignored if they don't exist in the root
environment.</p>
<h4 id="switching-off-the-sandbox"><a class="header" href="#switching-off-the-sandbox">Switching off the sandbox</a></h4>
<p>Per default, all binary benchmarks and the <code>before</code>, <code>after</code>, <code>setup</code> and <code>teardown</code> functions are
executed in a temporary directory. This behavior can be switched off at group-level:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>binary_benchmark_group!(
    name = my_exe_group;
    benchmark = |group: &amp;mut BinaryBenchmarkGroup| {
        group.sandbox(false);
    });
<span class="boring">}</span></code></pre></pre>
<h4 id="examples-1"><a class="header" href="#examples-1">Examples</a></h4>
<p>See the <a href="benchmark-tests/benches/test_bin_bench_groups.rs">test_bin_bench_groups</a> benchmark file of
this project for a working example.</p>
<h3 id="performance-regressions"><a class="header" href="#performance-regressions">Performance Regressions</a></h3>
<p>With Iai-Callgrind you can define limits for each event kinds over which a
performance regression can be assumed. There are no default regression checks
and you have to opt-in with a <code>RegressionConfig</code> at benchmark level or at a
global level with <a href="#command-line-arguments-and-environment-variables">Command-line arguments or Environment
variables</a>.</p>
<p>A performance regression check consists of an <code>EventKind</code> and a percentage over
which a regression is assumed. If the percentage is negative, then a regression
is assumed to be below this limit. The default <code>EventKind</code> is
<code>EventKind::Ir</code> with a value of <code>+10%</code>.For example, in a <a href="#library-benchmarks">Library
Benchmark</a>, let's overwrite the default limit with a global
limit of <code>+5%</code> for the total instructions executed (the <code>Ir</code> event kind):</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>main!(
    config = LibraryBenchmarkConfig::default()
        .regression(
            RegressionConfig::default()
                .limits([(EventKind::Ir, 5.0)])
        );
    library_benchmark_groups = some_group
);
<span class="boring">}</span></code></pre></pre>
<p>For example <a href="https://sqlite.org/cpu.html#performance_measurement">SQLite</a> uses
mainly cpu instructions to measure performance improvements (and regressions).</p>
<p>For more details on regression checks consult the iai-callgrind <a href="https://docs.rs/iai-callgrind/0.12.3/iai_callgrind/">docs</a>.</p>
<h3 id="valgrind-tools"><a class="header" href="#valgrind-tools">Valgrind Tools</a></h3>
<p>In addition to the default benchmarks, you can use the Iai-Callgrind framework
to run other Valgrind profiling <code>Tool</code>s like <code>DHAT</code>, <code>Massif</code> and the
experimental <code>BBV</code> but also <code>Memcheck</code>, <code>Helgrind</code> and <code>DRD</code> if you need to
check memory and thread safety of benchmarked code. See also the <a href="https://valgrind.org/docs/manual/manual.html">Valgrind User
Manual</a> for more details and
command line arguments. The additional tools can be specified in
<code>LibraryBenchmarkConfig</code>, <code>BinaryBenchmarkConfig</code> or <code>Run</code>. For example to run
<code>DHAT</code> for all library benchmarks:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use iai_callgrind::{
    library_benchmark, library_benchmark_group, main, LibraryBenchmarkConfig, Tool,
    ValgrindTool
};

#[library_benchmark]
fn some_func() {
    println!("Hello, World!");
}

library_benchmark_group!(name = some_group; benchmarks = some_func);

main!(
    config = LibraryBenchmarkConfig::default()
                .tool(Tool::new(ValgrindTool::DHAT));
    library_benchmark_groups = some_group
);
<span class="boring">}</span></code></pre></pre>
<p>All tools which produce an <code>ERROR SUMMARY</code> <code>(Memcheck, DRD, Helgrind)</code> have
<code>--error-exitcode=201</code> (<a href="https://valgrind.org/docs/manual/manual-core.html#manual-core.erropts">See
also</a>)
set, so if there are any errors, the benchmark run fails with <code>201</code>. You can
overwrite this default with</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>Tool::new(ValgrindTool::Memcheck).args("--error-exitcode=0")
<span class="boring">}</span></code></pre></pre>
<p>which would restore the default of <code>0</code> from valgrind.</p>
<h3 id="valgrind-client-requests"><a class="header" href="#valgrind-client-requests">Valgrind Client Requests</a></h3>
<p><code>iai-callgrind</code> ships with it's own interface to <a href="https://valgrind.org/docs/manual/manual-core-adv.html#manual-core-adv.clientreq">Valgrind's Client Request
Mechanism</a>.
<code>iai-callgrind's</code> client requests have (compared to the valgrind's client
requests used in <code>C</code> code) zero overhead on many targets which are also natively
supported by valgrind. My opinion may be biased, but compared to other crates
that offer an interface to valgrind's client requests, <code>iai-callgrind</code> provides
the most complete and best performant implementation.</p>
<p>Client requests are deactivated by default but can be activated with the
<code>client_requests</code> feature.</p>
<pre><code class="language-toml">[dev-dependencies]
iai-callgrind = { version = "0.12.3", features = ["client_requests"] }
</code></pre>
<p>If you need the client requests in your production code, you usually don't want
them to do anything when not running under valgrind with <code>iai-callgrind</code>
benchmarks. You can achieve that by adding <code>iai-callgrind</code> with the
<code>client_requests_defs</code> feature to your runtime dependencies and with the
<code>client_requests</code> feature to your <code>dev-dependencies</code> like so:</p>
<pre><code class="language-toml">[dependencies]
iai-callgrind = { version = "0.12.3", default-features = false, features = [
    "client_requests_defs"
] }

[dev-dependencies]
iai-callgrind = { version = "0.12.3", features = ["client_requests"] }
</code></pre>
<p>With just the <code>client_requests_defs</code> feature activated, the client requests
compile down to nothing and don't add any overhead to your production code. It
simply provides the "definitions", method signatures and macros without body.
Only with the activated <code>client_requests</code> feature they will be actually
executed. Note that the client requests do not depend on any other part of
<code>iai-callgrind</code>, so you could even use the client requests without the rest of
<code>iai-callgrind</code>.</p>
<p>Use them in your code for example like so:</p>
<pre><pre class="playground"><code class="language-rust edition2021">use iai_callgrind::client_requests;

fn main() {
    // Start callgrind event counting if not already started earlier
    client_requests::callgrind::start_instrumentation();

    // do something important

    // Toggle callgrind event counting off
    client_requests::callgrind::toggle_collect();
}</code></pre></pre>
<p>When building <code>iai-callgrind</code> with client requests, the valgrind header files
must exist in your standard include path (most of the time <code>/usr/include</code>). This
is usually the case if you've installed valgrind with your distribution's
package manager. If not, you can point the <code>IAI_CALLGRIND_VALGRIND_INCLUDE</code> or
<code>IAI_CALLGRIND_&lt;triple&gt;_VALGRIND_INCLUDE</code> environment variables to the include
path. So, if the headers can be found in
<code>/home/foo/repo/valgrind/{valgrind.h, callgrind.h, ...}</code>, the correct include
path would be <code>IAI_CALLGRIND_VALGRIND_INCLUDE=/home/foo/repo</code> (not
<code>/home/foo/repo/valgrind</code>)</p>
<p>This was just a small introduction, please see the
<a href="https://docs.rs/iai-callgrind/0.12.3/iai_callgrind/client_requests">docs</a> for
more details!</p>
<h3 id="flamegraphs"><a class="header" href="#flamegraphs">Flamegraphs</a></h3>
<p>Flamegraphs are opt-in and can be created if you pass a <code>FlamegraphConfig</code> to
the <code>BinaryBenchmarkConfig</code>, <code>Run</code> or <code>LibraryBenchmarkConfig</code>. Callgrind
flamegraphs are meant as a complement to valgrind's visualization tools
<code>callgrind_annotate</code> and <code>kcachegrind</code>.</p>
<p>Callgrind flamegraphs show the inclusive costs for functions and a single
<code>EventKind</code> (default is <code>EventKind::Ir</code>), similar to <code>callgrind_annotate</code> but in
a nicer (and clickable) way. Especially, differential flamegraphs facilitate a
deeper understanding of code sections which cause a bottleneck or a performance
regressions etc.</p>
<p>The produced flamegraph <code>*.svg</code> files are located next to the respective callgrind
output file in the <code>target/iai</code> directory.</p>
<h3 id="command-line-arguments-and-environment-variables"><a class="header" href="#command-line-arguments-and-environment-variables">Command-line arguments and environment variables</a></h3>
<p>It's possible to pass arguments to iai-callgrind separated by <code>--</code> (<code>cargo bench -- ARGS</code>). If you're running into the error <code>Unrecognized Option</code>, see the
<a href="#running-cargo-bench-results-in-an-unrecognized-option-error">FAQ</a>. For a
complete rundown of possible arguments, execute <code>cargo bench --bench &lt;benchmark&gt; -- --help</code>. Almost all command-line arguments have a corresponding environment
variable. The environment variables which don't have a corresponding
command-line argument are:</p>
<ul>
<li><code>IAI_CALLGRIND_COLOR</code>: Control the colored output of iai-callgrind. (Default
is <code>auto</code>)</li>
<li><code>IAI_CALLGRIND_LOG</code>: Define the log level (Default is <code>WARN</code>)</li>
</ul>
<h4 id="comparing-with-baselines"><a class="header" href="#comparing-with-baselines">Comparing with baselines</a></h4>
<p>Usually, two consecutive benchmark runs let iai-callgrind compare these two
runs. It's sometimes desirable to compare the current benchmark run against a
static reference, instead. For example, if you're working longer on the
implementation of a feature, you may wish to compare against a baseline from
another branch or the commit from which you started off hacking on your new
feature to make sure you haven't introduced performance regressions.
<code>iai-callgrind</code> offers such custom baselines. If you are familiar with
<a href="https://bheisler.github.io/criterion.rs/book/user_guide/command_line_options.html#baselines">criterion.rs</a>,
the following command line arguments should also be very familiar to you:</p>
<ul>
<li><code>--save-baseline=NAME</code>: Compare against the <code>NAME</code> baseline if present and
then overwrite it. (env: <code>IAI_CALLGRIND_SAVE_BASELINE</code>)</li>
<li><code>--baseline=NAME</code>: Compare against the <code>NAME</code> baseline without overwriting it
(env: <code>IAI_CALLGRIND_BASELINE</code>)</li>
<li><code>--load-baseline=NAME</code>: Load the <code>NAME</code> baseline as the <code>new</code> data set instead
of creating a new one. This options needs also <code>--baseline=NAME</code> to be
present. (env: <code>IAI_CALLGRIND_LOAD_BASELINE</code>)</li>
</ul>
<p>If <code>NAME</code> is not present, <code>NAME</code> defaults to <code>default</code>.</p>
<p>For example to create a static reference from the main branch and compare it:</p>
<pre><code class="language-shell">git checkout main
cargo bench --bench &lt;benchmark&gt; -- --save-baseline=main
git checkout feature
# ... HACK ... HACK
cargo bench --bench &lt;benchmark&gt; -- --baseline main
</code></pre>
<h4 id="customize-the-output-directory"><a class="header" href="#customize-the-output-directory">Customize the output directory</a></h4>
<p>Per default, all benchmark output files are stored under the
<code>$PROJECT_ROOT/target/iai</code> directory tree. This home directory can be changed
with the <code>IAI_CALLGRIND_HOME</code> environment variable or the command-line argument
<code>--home</code>. The command-line argument overwrites the value of the environment
variable. For example to store all files under the <code>/tmp/iai-callgrind</code>
directory you can use <code>IAI_CALLGRIND_HOME=/tmp/iai-callgrind</code> or <code>cargo bench -- --home=/tmp/iai-callgrind</code>.</p>
<p>If you're running the benchmarks on different targets, it's necessary to
separate the output files of the benchmark runs per target or else you could end
up comparing the benchmarks with the wrong target leading to strange results.
You can achieve this with different baselines per target, but it's much less
painful to separate the output files by target with the <code>--separate-targets</code>
command-line argument or setting the environment variable
<code>IAI_CALLGRIND_SEPARATE_TARGETS=yes</code>). The output directory structure simply
changes from
<code>target/iai/$PACKAGE_NAME/$BENCHMARK_FILE/$GROUP/$BENCH_FUNCTION.$BENCH_ID</code> to
<code>target/iai/$TARGET_TRIPLE/$PACKAGE_NAME/$BENCHMARK_FILE/$GROUP/$BENCH_FUNCTION.$BENCH_ID</code>.</p>
<p>For example the output directory of the following library benchmark assuming the
benchmark file name is <code>bench_file</code> in the package <code>my_package</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use iai_callgrind::{main, library_benchmark_group, library_benchmark};
use my_lib::my_function;

#[library_benchmark]
#[bench::short(10)]
fn bench_function(value: u64) -&gt; u64 {
    my_function(value)
}

library_benchmark_group!(
    name = bench_group;
    benchmarks = bench_function
);

main!(library_benchmark_groups = bench_group);
<span class="boring">}</span></code></pre></pre>
<p>Without <code>--separate-targets</code>:</p>
<p><code>target/iai/my_package/bench_file/bench_group/bench_function.short</code></p>
<p>and with <code>--separate-targets</code> assuming you're running the benchmark on the
<code>x86_64-unknown-linux-gnu</code> target:</p>
<p><code>target/iai/x86_64-unknown-linux-gnu/my_package/bench_file/bench_group/bench_function.short</code></p>
<h4 id="machine-readable-output"><a class="header" href="#machine-readable-output">Machine-readable output</a></h4>
<p>With <code>--output-format=default|json|pretty-json</code> (env:
<code>IAI_CALLGRIND_OUTPUT_FORMAT</code>) you can change the terminal output format to the
machine-readable json format. The json schema fully describing the json output
is stored in
<a href="./iai-callgrind-runner/schemas/summary.v2.schema.json">summary.v2.schema.json</a>.
Each line of json output (if not <code>pretty-json</code>) is a summary of a single
benchmark and you may want to combine all benchmarks in an array. You can do so
for example with <code>jq</code></p>
<p><code>cargo bench -- --output-format=json | jq -s</code></p>
<p>which transforms <code>{...}\n{...}</code> into <code>[{...},{...}]</code>.</p>
<p>Instead of or in addition to changing the terminal output, it's possible to save
a summary file for each benchmark with <code>--save-summary=json|pretty-json</code> (env:
<code>IAI_CALLGRIND_SAVE_SUMMARY</code>). The <code>summary.json</code> files are stored next to the
usual benchmark output files in the <code>target/iai</code> directory.</p>
<h4 id="other-output-options"><a class="header" href="#other-output-options">Other output options</a></h4>
<p>This section describes other command-line options and environment variables
which influence the terminal and logging output of iai-callgrind.</p>
<h5 id="show-terminal-output-of-benchmarks"><a class="header" href="#show-terminal-output-of-benchmarks">Show terminal output of benchmarks</a></h5>
<p>Per default, all terminal output is captured and therefore not shown during a
benchmark run. To show any captured output, you can use
<code>IAI_CALLGRIND_LOG=info</code>. Another possibility is, to tell <code>iai-callgrind</code> to not
capture output with the <code>--nocapture</code> (env: <code>IAI_CALLGRIND_NOCAPTURE</code>) option.
This is currently restricted to the <code>callgrind</code> run to prevent showing the same
output multiple times. So, any terminal output of other tool runs (see also
<a href="#valgrind-tools">Valgrind Tools</a>) is still captured.</p>
<p>The <code>--nocapture</code> flag takes the special values <code>stdout</code> and <code>stderr</code> in
addition to <code>true</code> and <code>false</code>. In the <code>--nocapture=stdout</code> case, terminal
output to <code>stdout</code> is not captured and shown during the benchmark run but output
to <code>stderr</code> is discarded. Likewise, <code>--nocapture=stderr</code> shows terminal output
to <code>stderr</code> but discards output to <code>stdout</code>.</p>
<p>For example a library benchmark <code>benches/my_benchmark.rs</code></p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>use iai_callgrind::{main, library_benchmark_group, library_benchmark};

fn my_teardown(value: u64) {
    eprintln!("Error output during teardown: {value}");
}

fn to_be_benchmarked(value: u64) -&gt; u64 {
    println!("Output to stdout: {value}");
    value + 10
}

#[library_benchmark]
#[bench::some_id(args = (10), teardown = my_teardown]
fn my_bench(value: u64) -&gt; u64 {
    to_be_benchmarked(value)
}

library_benchmark_group!(
    name = my_bench_group;
    benchmarks = my_bench
);

main!(library_benchmark_groups = my_bench_group);
<span class="boring">}</span></code></pre></pre>
<p>If the above benchmark is run with <code>cargo bench --bench my_benchmark -- --nocapture</code>, the output of iai-callgrind will look like this (The values of
Instructions and so on don't matter here and are made up)</p>
<pre><code class="language-text">my_benchmark::my_bench_group::my_bench some_id:10
Output to stdout: 10
Error output during teardown: 20
- end of stdout/stderr
  Instructions:              331082|N/A             (*********)
  L1 Hits:                   442452|N/A             (*********)
  L2 Hits:                      720|N/A             (*********)
  RAM Hits:                    3926|N/A             (*********)
  Total read+write:          447098|N/A             (*********)
  Estimated Cycles:          583462|N/A             (*********)
</code></pre>
<p>Note that independently of the value of the <code>--nocapture</code> option, all logging
output of a valgrind tool itself is stored in files in the output directory of
the benchmark. Since <code>iai-callgrind</code> needs the logging output of valgrind tools
stored in files, there is no option to disable the creation of these log files.
But, if anything goes sideways you might be glad to have the log files around.</p>
<h5 id="changing-the-color-output"><a class="header" href="#changing-the-color-output">Changing the color output</a></h5>
<p>The terminal output is colored per default but follows the value for the
<code>IAI_CALLGRIND_COLOR</code> environment variable. If <code>IAI_CALLGRIND_COLOR</code> is not set,
<code>CARGO_TERM_COLOR</code> is also tried. Accepted values are: <code>always</code>, <code>never</code>, <code>auto</code>
(default). So, disabling colors can be achieved with setting
<code>IAI_CALLGRIND_COLOR</code> or <code>CARGO_TERM_COLOR=never</code>.</p>
<h5 id="changing-the-logging-output"><a class="header" href="#changing-the-logging-output">Changing the logging output</a></h5>
<p>This library uses <a href="https://crates.io/crates/env_logger">env_logger</a> and the
default logging level <code>WARN</code>. To set the logging level to something different,
set the environment variable <code>IAI_CALLGRIND_LOG</code> for example to
<code>IAI_CALLGRIND_LOG=DEBUG</code>. Accepted values are: <code>error</code>, <code>warn</code> (default),
<code>info</code>, <code>debug</code>, <code>trace</code>. The logging output is colored per default but follows
the settings of <code>IAI_CALLGRIND_COLOR</code> and <code>CARGO_TERM_COLOR</code> (In this order of
precedence). See also the <a href="https://docs.rs/env_logger/latest">documentation</a> of
<code>env_logger</code>.</p>
<h3 id="features-and-differences-to-iai"><a class="header" href="#features-and-differences-to-iai">Features and differences to Iai</a></h3>
<p>This crate is built on the same idea like the original Iai, but over the time applied a lot of
improvements. The biggest difference is, that it uses Callgrind under the hood instead of
Cachegrind.</p>
<h4 id="more-stable-metrics"><a class="header" href="#more-stable-metrics">More stable metrics</a></h4>
<p>Iai-Callgrind has even more precise and stable metrics across different systems. It achieves this by</p>
<ul>
<li>only counting events of function calls within the benchmarking function. This behavior virtually
encapsulates the benchmark function and separates the benchmark from the surrounding code.</li>
<li>separating the iai library with the main macro from the actual runner. This is the reason for the
extra installation step of <code>iai-callgrind-runner</code> but before this separation even small changes in
the iai library had effects on the benchmarks under test.</li>
</ul>
<p>Below a local run of one of the benchmarks of this library</p>
<pre><code class="language-shell">$ cd iai-callgrind
$ cargo bench --bench test_lib_bench_readme_example_fibonacci
test_lib_bench_readme_example_fibonacci::bench_fibonacci_group::bench_fibonacci short:10
  Instructions:                1733|N/A             (*********)
  L1 Hits:                     2359|N/A             (*********)
  L2 Hits:                        0|N/A             (*********)
  RAM Hits:                       2|N/A             (*********)
  Total read+write:            2361|N/A             (*********)
  Estimated Cycles:            2429|N/A             (*********)
test_lib_bench_readme_example_fibonacci::bench_fibonacci_group::bench_fibonacci long:30
  Instructions:            26214733|N/A             (*********)
  L1 Hits:                 35638617|N/A             (*********)
  L2 Hits:                        0|N/A             (*********)
  RAM Hits:                       4|N/A             (*********)
  Total read+write:        35638621|N/A             (*********)
  Estimated Cycles:        35638757|N/A             (*********)
</code></pre>
<p>For comparison, the output of the same benchmark but in the github CI is
producing the same results. Usually, there's almost no difference between a CI
run and a local run what makes benchmark runs and performance improvements of
the benchmarked code even more comparable across systems.</p>
<h4 id="cleaner-output-of-valgrinds-annotation-tools"><a class="header" href="#cleaner-output-of-valgrinds-annotation-tools">Cleaner output of Valgrind's annotation tools</a></h4>
<p>The now obsolete calibration run needed with Iai has just fixed the summary output of Iai itself,
but the output of <code>cg_annotate</code> was still cluttered by the setup functions and metrics. The
<code>callgrind_annotate</code> output produced by Iai-Callgrind is far cleaner and centered on the actual
function under test.</p>
<h4 id="rework-the-metrics-output"><a class="header" href="#rework-the-metrics-output">Rework the metrics output</a></h4>
<p>The statistics of the benchmarks are mostly not compatible with the original Iai anymore although
still related. They now also include some additional information:</p>
<pre><code class="language-text">test_lib_bench_readme_example_fibonacci::bench_fibonacci_group::bench_fibonacci short:10
  Instructions:                1733|N/A             (*********)
  L1 Hits:                     2359|N/A             (*********)
  L2 Hits:                        0|N/A             (*********)
  RAM Hits:                       2|N/A             (*********)
  Total read+write:            2361|N/A             (*********)
  Estimated Cycles:            2429|N/A             (*********)
</code></pre>
<p>There is an additional line <code>Total read+write</code> which summarizes all event counters of the lines with
<code>Hits</code> above it and the <code>L1 Accesses</code> line changed to <code>L1 Hits</code>.</p>
<p>In detail:</p>
<p><code>Total read+write = L1 Hits + L2 Hits + RAM Hits</code>.</p>
<p>The formula for the <code>Estimated Cycles</code> hasn't changed and uses Itamar Turner-Trauring's formula from
<a href="https://pythonspeed.com/articles/consistent-benchmarking-in-ci/">https://pythonspeed.com/articles/consistent-benchmarking-in-ci/</a>:</p>
<p><code>Estimated Cycles = L1 Hits + 5 × (L2 Hits) + 35 × (RAM Hits)</code></p>
<p>For further details about how the caches are simulated and more, see the documentation of
<a href="https://valgrind.org/docs/manual/cg-manual.html">Callgrind</a></p>
<h3 id="what-hasnt-changed"><a class="header" href="#what-hasnt-changed">What hasn't changed</a></h3>
<p>Iai-Callgrind cannot completely remove the influences of setup changes. However, these effects
shouldn't be significant anymore.</p>
<h3 id="faq"><a class="header" href="#faq">FAQ</a></h3>
<h4 id="im-getting-the-error-sentinel--not-found"><a class="header" href="#im-getting-the-error-sentinel--not-found">I'm getting the error <code>Sentinel ... not found</code></a></h4>
<p>You've most likely disabled creating debug symbols in your cargo <code>bench</code>
profile. This can originate in an option you've added to the <code>release</code> profile
since the <code>bench</code> profile inherits the <code>release</code> profile. For example, if you've
added <code>strip = true</code> to your <code>release</code> profile which is perfectly fine, you need
to disable this option in your <code>bench</code> profile to be able to run <code>iai-callgrind</code>
benchmarks. See also the <a href="#benchmarking">Benchmarking</a> section for a more
thorough example.</p>
<h4 id="running-cargo-bench-results-in-an-unrecognized-option-error"><a class="header" href="#running-cargo-bench-results-in-an-unrecognized-option-error">Running <code>cargo bench</code> results in an "Unrecognized Option" error</a></h4>
<p>For <code>cargo bench -- --some-valid-arg</code> to work you can either specify the benchmark with
<code>--bench BENCHMARK</code>, for example <code>cargo bench --bench my_iai_benchmark -- --callgrind-args="--collect-bus=yes"</code> or add the following to your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[lib]
bench = false
</code></pre>
<p>and if you have binaries</p>
<pre><code class="language-toml">[[bin]]
name = "my-binary"
path = "src/bin/my-binary.rs"
bench = false
</code></pre>
<p>Setting <code>bench = false</code> disables the creation of the implicit default <code>libtest</code>
harness which is added even if you haven't used <code>#[bench]</code> functions in your
library or binary. Naturally, the default harness doesn't know of the
<code>iai-callgrind</code> arguments and aborts execution printing the <code>Unrecognized Option</code> error.</p>
<p>If you cannot or don't want to add <code>bench = false</code> to your <code>Cargo.toml</code>, you can
alternatively use environment variables. For every <a href="#command-line-arguments-and-environment-variables">command-line
argument</a> exists a
corresponding environment variable.</p>
<h3 id="contributing"><a class="header" href="#contributing">Contributing</a></h3>
<p>A guideline about contributing to iai-callgrind can be found in the
<a href="./CONTRIBUTING.html">CONTRIBUTING.md</a> file.</p>
<h3 id="see-also"><a class="header" href="#see-also">See also</a></h3>
<ul>
<li>The user guide of the original Iai: <a href="https://bheisler.github.io/criterion.rs/book/iai/iai.html">https://bheisler.github.io/criterion.rs/book/iai/iai.html</a></li>
<li>A comparison of criterion-rs with Iai: <a href="https://github.com/bheisler/iai#comparison-with-criterion-rs">https://github.com/bheisler/iai#comparison-with-criterion-rs</a></li>
</ul>
<h3 id="credits"><a class="header" href="#credits">Credits</a></h3>
<p>Iai-Callgrind is forked from <a href="https://github.com/bheisler/iai">https://github.com/bheisler/iai</a> and was originally written by Brook
Heisler (@bheisler).</p>
<p>Iai-Callgrind wouldn't be possible without <a href="https://valgrind.org/">Valgrind</a>.</p>
<h3 id="license"><a class="header" href="#license">License</a></h3>
<p>Iai-Callgrind is like Iai dual licensed under the Apache 2.0 license and the MIT
license at your option.</p>
<p>According to <a href="https://valgrind.org/docs/manual/manual-core-adv.html#manual-core-adv.clientreq">Valgrind's documentation</a>:</p>
<blockquote>
<p>The Valgrind headers, unlike most of the rest of
the code, are under a BSD-style license so you may include them without worrying
about license incompatibility.</p>
</blockquote>
<p>We have included the original license where we make use of the original header
files.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="intro.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="intro.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="js/dropdown.js"></script>


    </div>
    </body>
</html>
