<!DOCTYPE HTML>
<html lang="en" class="light" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Iai-Callgrind Guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->
        
        <meta name="description" content="Iai-Callgrind, a high-precision and consistent one-shot benchmarking framework/harness for Rust">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="css/my.css">
        <link rel="stylesheet" href="css/dropdown.css">

    </head>
    <body class="sidebar-visible no-js">
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('light')
            html.classList.add(theme);
            var body = document.querySelector('body');
            body.classList.remove('no-js')
            body.classList.add('js');
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var body = document.querySelector('body');
            var sidebar = null;
            var sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            sidebar_toggle.checked = sidebar === 'visible';
            body.classList.remove('sidebar-visible');
            body.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="intro.html"><strong aria-hidden="true">1.</strong> Introduction</a></li><li class="chapter-item expanded "><a href="getting_help.html"><strong aria-hidden="true">2.</strong> Getting Help</a></li><li class="chapter-item expanded affix "><li class="part-title">Installation</li><li class="chapter-item expanded "><a href="installation/prerequisites.html"><strong aria-hidden="true">3.</strong> Prerequisites</a></li><li class="chapter-item expanded "><a href="installation/iai_callgrind.html"><strong aria-hidden="true">4.</strong> Iai-Callgrind</a></li><li class="chapter-item expanded affix "><li class="part-title">Benchmarks</li><li class="chapter-item expanded "><a href="benchmarks/overview.html"><strong aria-hidden="true">5.</strong> Overview</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks.html"><strong aria-hidden="true">6.</strong> Library Benchmarks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/important.html"><strong aria-hidden="true">6.1.</strong> Important default behaviour</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/quickstart.html"><strong aria-hidden="true">6.2.</strong> Quickstart</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/anatomy.html"><strong aria-hidden="true">6.3.</strong> Anatomy of a library benchmark</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/macros.html"><strong aria-hidden="true">6.4.</strong> The macros in more detail</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/setup_and_teardown.html"><strong aria-hidden="true">6.5.</strong> setup and teardown</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/multiple_benches.html"><strong aria-hidden="true">6.6.</strong> Specify multiple benches at once</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/generic.html"><strong aria-hidden="true">6.7.</strong> Generic benchmark functions</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/compare_by_id.html"><strong aria-hidden="true">6.8.</strong> Comparing benchmark functions</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/configuration.html"><strong aria-hidden="true">6.9.</strong> Configuration</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/custom_entry_point.html"><strong aria-hidden="true">6.10.</strong> Custom entry points</a></li><li class="chapter-item expanded "><a href="benchmarks/library_benchmarks/examples.html"><strong aria-hidden="true">6.11.</strong> More Examples, please!</a></li></ol></li><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks.html"><strong aria-hidden="true">7.</strong> Binary Benchmarks</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks/important.html"><strong aria-hidden="true">7.1.</strong> Important default behaviour</a></li><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks/quickstart.html"><strong aria-hidden="true">7.2.</strong> Quickstart</a></li><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks/differences.html"><strong aria-hidden="true">7.3.</strong> Differences to library benchmarks</a></li><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks/stdin_and_pipe.html"><strong aria-hidden="true">7.4.</strong> The Command's stdin and simulating piped input</a></li><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks/configuration.html"><strong aria-hidden="true">7.5.</strong> Configuration</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks/configuration/sandbox.html"><strong aria-hidden="true">7.5.1.</strong> Sandbox</a></li><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks/configuration/exit_code.html"><strong aria-hidden="true">7.5.2.</strong> Configure the exit code of the Command</a></li></ol></li><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks/low_level.html"><strong aria-hidden="true">7.6.</strong> Low-level api</a></li><li class="chapter-item expanded "><a href="benchmarks/binary_benchmarks/examples.html"><strong aria-hidden="true">7.7.</strong> More examples needed?</a></li></ol></li><li class="chapter-item expanded "><a href="regressions.html"><strong aria-hidden="true">8.</strong> Performance Regressions</a></li><li class="chapter-item expanded "><a href="tools.html"><strong aria-hidden="true">9.</strong> Other Valgrind Tools</a></li><li class="chapter-item expanded "><a href="client_requests.html"><strong aria-hidden="true">10.</strong> Valgrind Client Requests</a></li><li class="chapter-item expanded "><a href="flamegraphs.html"><strong aria-hidden="true">11.</strong> Callgrind Flamegraphs</a></li><li class="chapter-item expanded affix "><li class="part-title">Command-line and environment variables</li><li class="chapter-item expanded "><a href="cli_and_env/basics.html"><strong aria-hidden="true">12.</strong> Basic usage</a></li><li class="chapter-item expanded "><a href="cli_and_env/baselines.html"><strong aria-hidden="true">13.</strong> Comparing with baselines</a></li><li class="chapter-item expanded "><a href="cli_and_env/output.html"><strong aria-hidden="true">14.</strong> Controlling the output of Iai-Callgrind</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="cli_and_env/output/out_directory.html"><strong aria-hidden="true">14.1.</strong> Customize the output directory</a></li><li class="chapter-item expanded "><a href="cli_and_env/output/machine_readable.html"><strong aria-hidden="true">14.2.</strong> Machine-readable output</a></li><li class="chapter-item expanded "><a href="cli_and_env/output/terminal_output.html"><strong aria-hidden="true">14.3.</strong> Showing terminal output of benchmarks</a></li><li class="chapter-item expanded "><a href="cli_and_env/output/color.html"><strong aria-hidden="true">14.4.</strong> Changing the color output</a></li><li class="chapter-item expanded "><a href="cli_and_env/output/logging.html"><strong aria-hidden="true">14.5.</strong> Changing the logging output</a></li></ol></li><li class="chapter-item expanded "><li class="part-title">Troubleshooting</li><li class="chapter-item expanded "><a href="troubleshooting/im-getting-the-error-sentinel-not-found.html"><strong aria-hidden="true">15.</strong> I'm getting the error Sentinel ... not found</a></li><li class="chapter-item expanded "><a href="troubleshooting/running-cargo-bench-results-in-an-unrecognized-option-error.html"><strong aria-hidden="true">16.</strong> Running cargo bench results in an "Unrecognized Option" error</a></li><li class="chapter-item expanded affix "><li class="part-title">Comparison</li><li class="chapter-item expanded "><a href="comparison/criterion.html"><strong aria-hidden="true">17.</strong> Criterion</a></li><li class="chapter-item expanded "><a href="comparison/iai.html"><strong aria-hidden="true">18.</strong> Iai</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                        <div class="dropdown">
                          <button onclick="versionDropdown()" class="icon-button version-dropdown-button" type="button">Version</button>
                          <div id="version-dropdown" class="version-dropdown-content">
                            <a href="../../latest/html/index.html">Latest</a>
                            <hr class="sidbar-spacer">
                            <script src="../../versions.js"></script>
                          </div>
                        </div>
                    </div>

                    <h1 class="menu-title">Iai-Callgrind Guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/iai-callgrind/iai-callgrind" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<p>This is the guide for Iai-Callgrind, a benchmarking framework/harness which uses
<a href="https://valgrind.org/docs/manual/cl-manual.html">Valgrind's Callgrind</a> and
other Valgrind tools like DHAT, Massif, ... to provide extremely accurate and
consistent measurements of Rust code, making it perfectly suited to run in
environments like a CI.</p>
<p>Iai_Callgrind is fully documented in this guide and in the api documentation at
<a href="https://docs.rs/iai-callgrind/latest/iai_callgrind/">docs.rs</a>.</p>
<p>Iai-Callgrind is</p>
<ul>
<li><strong>Precise</strong>: High-precision measurements of <code>Instruction</code> counts and many
other metrics allow you to reliably detect very small optimizations and
regressions of your code.</li>
<li><strong>Consistent</strong>: Iai-Callgrind can take accurate measurements even in
virtualized CI environments and make them comparable between different systems
completely negating the noise of the environment.</li>
<li><strong>Fast</strong>: Each benchmark is only run once, which is usually much faster than
benchmarks which measure execution and wall-clock time. Benchmarks measuring
the wall-clock time have to be run many times to increase their accuracy,
detect outliers, filter out noise, etc.</li>
<li><strong>Visualizable</strong>: Iai-Callgrind generates a Callgrind (DHAT, ...) profile of
the benchmarked code and can be configured to create flamegraph-like charts
from Callgrind metrics. In general, all Valgrind-compatible tools like
<a href="https://valgrind.org/docs/manual/cl-manual.html#cl-manual.callgrind_annotate-options">callgrind_annotate</a>,
<a href="https://kcachegrind.github.io/html/Home.html">kcachegrind</a> or <code>dh_view.html</code>
and others to analyze the results in detail are fully supported.</li>
<li><strong>Easy</strong>: The API for setting up benchmarks is easy to use and allows you to
quickly create concise and clear benchmarks. Focus more on profiling and your
code than on the framework.</li>
</ul>
<h2 id="design-philosophy-and-goals"><a class="header" href="#design-philosophy-and-goals">Design philosophy and goals</a></h2>
<p>Iai-Callgrind benchmarks are designed to be runnable with <code>cargo bench</code>. The
benchmark files are expanded to a benchmarking harness which replaces the native
benchmark harness of <code>rust</code>. Iai-Callgrind is a profiling framework that can
quickly and reliably detect performance regressions and optimizations even in
noisy environments with a precision that is impossible to achieve with
wall-clock time based benchmarks. At the same time, we want to abstract the
complicated parts and repetitive tasks away and provide an easy to use and
intuitive api. Concentrate more on profiling and your code than on the
framework!</p>
<h2 id="when-not-to-use-iai-callgrind"><a class="header" href="#when-not-to-use-iai-callgrind">When not to use Iai-Callgrind</a></h2>
<p>Although Iai-Callgrind is useful in many projects, there are cases where
Iai-Callgrind is not a good fit.</p>
<ul>
<li>If you need wall-clock times, Iai-Callgrind cannot help you much. The
estimation of cpu cycles merely correlates to wall-clock times but is not a
replacement for wall-clock times. The cycles estimation is primarily designed
to be a relative metric to be used for comparison.</li>
<li>Iai-Callgrind cannot be run on Windows and platforms not supported by
Valgrind.</li>
</ul>
<h2 id="improving-iai-callgrind"><a class="header" href="#improving-iai-callgrind">Improving Iai-Callgrind</a></h2>
<p>No one's perfect!</p>
<p>You want to share your experience with Iai-Callgrind and have a recipe that
might be useful for others and fits into this guide? You have an idea for a new
feature, are missing a functionality or have found a bug? We would love to here
about it. You want to contribute and hack on Iai-Callgrind?</p>
<p>Please don't hesitate to <a href="https://github.com/iai-callgrind/iai-callgrind/issues">open an
issue</a>.</p>
<p>You want to hack on this guide? The source code of this book lives in <a href="https://github.com/iai-callgrind/iai-callgrind/tree/main/docs">the docs
subdirectory</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="getting-help"><a class="header" href="#getting-help">Getting Help</a></h1>
<p>Reach out to us on <a href="https://github.com/iai-callgrind/iai-callgrind/discussions">Github
Discussions</a> or open
an <a href="https://github.com/iai-callgrind/iai-callgrind/issues">Issue</a> in the
<a href="https://github.com/iai-callgrind/iai-callgrind">Iai-Callgrind
Repository</a>. Check the
open and closed issues in the issue board, maybe you can already find a solution
to your problem there.</p>
<p>The api documentation can be found on
<a href="https://docs.rs/iai-callgrind/latest/iai_callgrind/">docs.rs</a> but you might
also want to check out the <code>Troubleshooting</code> section in the sidebar of this
guide.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h1>
<p>In order to use Iai-Callgrind, you must have <a href="https://www.valgrind.org">Valgrind</a> installed. This
means that Iai-Callgrind cannot be used on platforms that are not supported by Valgrind.</p>
<h2 id="debug-symbols"><a class="header" href="#debug-symbols">Debug Symbols</a></h2>
<p>It's required to run the Iai-Callgrind benchmarks with debugging symbols
switched on. For example in your <code>~/.cargo/config</code> or your project's
<code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[profile.bench]
debug = true
</code></pre>
<p>Now, all benchmarks which are run with <code>cargo bench</code> include the debug symbols.
(See also <a href="https://doc.rust-lang.org/cargo/reference/profiles.html">Cargo
Profiles</a> and <a href="https://doc.rust-lang.org/cargo/reference/config.html">Cargo
Config</a>).</p>
<p>It's required that settings like <code>strip = true</code> or other configuration options
stripping the debug symbols need to be disabled explicitly for the <code>bench</code>
profile if you have changed this option for the <code>release</code> profile. For example:</p>
<pre><code class="language-toml">[profile.release]
strip = true

[profile.bench]
debug = true
strip = false
</code></pre>
<h2 id="valgrind-client-requests"><a class="header" href="#valgrind-client-requests">Valgrind Client Requests</a></h2>
<p>If you want to make use of the mighty <a href="https://valgrind.org/docs/manual/manual-core-adv.html#manual-core-adv.clientreq">Valgrind Client Request
Mechanism</a>
shipped with Iai-Callgrind, you also need <code>libclang</code> (clang &gt;= 5.0) installed.
See also the requirements of
<a href="https://rust-lang.github.io/rust-bindgen/requirements.html">bindgen</a> and of
<a href="https://github.com/rust-lang/cc-rs">cc</a>.</p>
<p>More details on the usage and requirements of Valgrind Client Requests in
<a href="installation/../client_requests.html">this</a> chapter of the guide.</p>
<h2 id="installation-of-valgrind"><a class="header" href="#installation-of-valgrind">Installation of Valgrind</a></h2>
<p>Iai-Callgrind is intentionally independent from a specific version of valgrind.
However, Iai-Callgrind was only tested with versions of valgrind &gt;= <code>3.20.0</code>. It
is therefore highly recommended to use a recent version of valgrind. Bugs get
fixed, the supported platforms are expanded ... Also, if you want or need to,
<a href="https://sourceware.org/git/?p=valgrind.git;a=blob;f=README;h=eabcc6ad88c8cab6dfe73cfaaaf5543023c2e941;hb=HEAD">building valgrind from
source</a>
is usually a straight-forward process. Just make sure the <code>valgrind</code> binary is
in your <code>$PATH</code> so that Iai-callgrind can find it.</p>
<h3 id="installation-of-valgrind-with-your-package-manager"><a class="header" href="#installation-of-valgrind-with-your-package-manager">Installation of valgrind with your package manager</a></h3>
<h4 id="alpine-linux"><a class="header" href="#alpine-linux">Alpine Linux</a></h4>
<pre><code class="language-bash">apk add just
</code></pre>
<h4 id="arch-linux"><a class="header" href="#arch-linux">Arch Linux</a></h4>
<pre><code class="language-bash">pacman -Sy valgrind
</code></pre>
<h4 id="debianubuntu"><a class="header" href="#debianubuntu">Debian/Ubuntu</a></h4>
<pre><code class="language-bash">apt-get install valgrind
</code></pre>
<h4 id="fedora-linux"><a class="header" href="#fedora-linux">Fedora Linux</a></h4>
<pre><code class="language-bash">dnf install valgrind
</code></pre>
<h4 id="freebsd"><a class="header" href="#freebsd">FreeBSD</a></h4>
<pre><code class="language-bash">pkg install valgrind
</code></pre>
<h4 id="valgrind-is-available-for-the-following-distributions"><a class="header" href="#valgrind-is-available-for-the-following-distributions">Valgrind is available for the following distributions</a></h4>
<p><a href="https://repology.org/project/valgrind/versions"><img src="https://repology.org/badge/vertical-allrepos/valgrind.svg" alt="Packaging status" /></a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="iai-callgrind"><a class="header" href="#iai-callgrind">Iai-Callgrind</a></h1>
<p>Iai-Callgrind is divided into the library <code>iai-callgrind</code> and the benchmark runner
<code>iai-callgrind-runner</code>.</p>
<h2 id="installation-of-the-library"><a class="header" href="#installation-of-the-library">Installation of the library</a></h2>
<p>To start with Iai-Callgrind, add the following to your <code>Cargo.toml</code> file:</p>
<pre><code class="language-toml">[dev-dependencies]
iai-callgrind = "0.13.3"
</code></pre>
<p>or run</p>
<pre><code class="language-bash">cargo add --dev iai-callgrind@0.13.3
</code></pre>
<h2 id="installation-of-the-benchmark-runner"><a class="header" href="#installation-of-the-benchmark-runner">Installation of the benchmark runner</a></h2>
<p>To be able to run the benchmarks you'll also need the <code>iai-callgrind-runner</code>
binary installed somewhere in your <code>$PATH</code>. Otherwise, there is no need to
interact with <code>iai-callgrind-runner</code> as it is just an implementation detail.</p>
<h3 id="from-source"><a class="header" href="#from-source">From Source</a></h3>
<pre><code class="language-shell">cargo install --version 0.13.3 iai-callgrind-runner
</code></pre>
<p>There's also the possibility to install the binary somewhere else and point the
<code>IAI_CALLGRIND_RUNNER</code> environment variable to the absolute path of the
<code>iai-callgrind-runner</code> binary like so:</p>
<pre><code class="language-shell">cargo install --version 0.13.3 --root /tmp iai-callgrind-runner
IAI_CALLGRIND_RUNNER=/tmp/bin/iai-callgrind-runner cargo bench --bench my-bench
</code></pre>
<h3 id="binstall"><a class="header" href="#binstall">Binstall</a></h3>
<p>The <code>iai-callgrind-runner</code> binary is
<a href="https://github.com/iai-callgrind/iai-callgrind/releases/tag/v0.13.3">pre-built</a>
for most platforms supported by valgrind and easily installable with
<a href="https://github.com/cargo-bins/cargo-binstall">binstall</a></p>
<pre><code class="language-shell">cargo binstall iai-callgrind-runner@0.13.3
</code></pre>
<h2 id="updating"><a class="header" href="#updating">Updating</a></h2>
<p>When updating the <code>iai-callgrind</code> library, you'll also need to update
<code>iai-callgrind-runner</code> and vice-versa or else the benchmark runner will exit
with an error.</p>
<h3 id="in-the-github-ci"><a class="header" href="#in-the-github-ci">In the Github CI</a></h3>
<p>Since the <code>iai-callgrind-runner</code> version must match the <code>iai-callgrind</code> library
version it's best to automate this step in the CI. A job step in the github
actions CI could look like this</p>
<pre><code class="language-yaml">- name: Install iai-callgrind-runner
  run: |
    version=$(cargo metadata --format-version=1 |\
      jq '.packages[] | select(.name == "iai-callgrind").version' |\
      tr -d '"'
    )
    cargo install iai-callgrind-runner --version $version
</code></pre>
<p>Or, speed up the overall installation time with <code>binstall</code> using the
<a href="https://github.com/taiki-e/install-action">taiki-e/install-action</a></p>
<pre><code class="language-yaml">- uses: taiki-e/install-action@cargo-binstall
- name: Install iai-callgrind-runner
  run: |
    version=$(cargo metadata --format-version=1 |\
      jq '.packages[] | select(.name == "iai-callgrind").version' |\
      tr -d '"'
    )
    cargo binstall --no-confirm iai-callgrind-runner --version $version
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="overview"><a class="header" href="#overview">Overview</a></h1>
<p>Iai-Callgrind can be used to benchmark the <a href="benchmarks/./library_benchmarks.html">library</a>
and <a href="benchmarks/./binary_benchmarks.html">binary</a> of your project's crates. Library and
binary benchmarks are treated differently by Iai-Callgrind and cannot be
intermixed in the same benchmark file. This is indeed a feature and helps
keeping things organized. Having different and multiple benchmark files for
library and binary benchmarks is no problem for Iai-Callgrind and is usually a
good idea anyways. Having benchmarks for different binaries in the same
benchmark file however is fully supported.</p>
<p>Head over to the <a href="benchmarks/./library_benchmarks/quickstart.html">Quickstart</a> section of
library benchmarks if you want to start benchmarking your library functions or
to the <a href="benchmarks/./binary_benchmarks/quickstart.html">Quickstart</a> section of binary
benchmarks if you want to start benchmarking your crate's binary (binaries).</p>
<h2 id="binary-benchmarks-vs-library-benchmarks"><a class="header" href="#binary-benchmarks-vs-library-benchmarks">Binary Benchmarks vs Library Benchmarks</a></h2>
<p>Almost all binary benchmarks can be written as library benchmarks. For example,
if you have a <code>main.rs</code> file of your binary, which basically looks like this</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">mod my_lib { pub fn run() {} }
</span>use my_lib::run;

fn main() {
    run();
}</code></pre></pre>
<p>you could also choose to benchmark the library function <code>my_lib::run</code> in a
library benchmark instead of the binary in a binary benchmark. There's no real
downside to either of the benchmark schemes and which scheme you want to use
heavily depends on the structure of your binary. As a maybe obvious rule of
thumb, micro-benchmarks of specific functions should go into library benchmarks
and macro-benchmarks into binary benchmarks. Generally, choose the closest
access point to the program point you actually want to benchmark.</p>
<p>You should always choose binary benchmarks over library benchmarks if you want
to benchmark the behaviour of the executable if the input comes from a pipe
since this feature is exclusive to binary benchmarks. See <a href="benchmarks/./binary_benchmarks/stdin_and_pipe.html">The Command's stdin
and simulating piped input</a> for more.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="library-benchmarks"><a class="header" href="#library-benchmarks">Library Benchmarks</a></h1>
<p>You want to dive into benchmarking your library? No problem. Best start with the
<a href="benchmarks/./library_benchmarks/quickstart.html">Quickstart</a> section and then go through the
examples in the other sections of this guide. If you need more examples see
<a href="benchmarks/./library_benchmarks/examples.html">here</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="important-default-behaviour"><a class="header" href="#important-default-behaviour">Important default behaviour</a></h1>
<p>The environment variables are cleared before running a library benchmark. Have a
look into the <a href="benchmarks/library_benchmarks/./configuration.html">Configuration</a> section if you need to change that
behavior.</p>
<p>Per default, the benchmarks run with cache simulation switched on. This adds
additional run time costs. If you don't need the cache metrics and estimation of
cycles, yan can easily switch cache simulation off with</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::LibraryBenchmarkConfig;

LibraryBenchmarkConfig::default().raw_callgrind_args(["--cache-sim=no"]);
<span class="boring">}</span></code></pre></pre>
<p>For example to switch off cache simulation for all benchmarks in the same file:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn fibonacci(a: u64) -&gt; u64 { a } }
</span>use iai_callgrind::{
    main, library_benchmark_group, library_benchmark, LibraryBenchmarkConfig
};
use std::hint::black_box;

#[library_benchmark]
fn bench_fibonacci() -&gt; u64 {
    black_box(my_lib::fibonacci(10))
}

library_benchmark_group!(name = fibonacci_group; benchmarks = bench_fibonacci);

<span class="boring">fn main() {
</span>main!(
    config = LibraryBenchmarkConfig::default().raw_callgrind_args(["--cache-sim=no"]);
    library_benchmark_groups = fibonacci_group
);
<span class="boring">}</span></code></pre></pre>
<p>If you're new to Iai-Callgrind and don't know what the above means, don't panic.
Jump to <a href="benchmarks/library_benchmarks/./quickstart.html">Quickstart</a> and read through the first few chapters,
and you're ready to benchmark with Iai-Callgrind like a pro.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable MD041 MD033 -->
<h1 id="quickstart"><a class="header" href="#quickstart">Quickstart</a></h1>
<p>Create a file <code>$WORKSPACE_ROOT/benches/library_benchmark.rs</code> and add</p>
<pre><code class="language-toml">[[bench]]
name = "library_benchmark"
harness = false
</code></pre>
<p>to your <code>Cargo.toml</code>. <code>harness = false</code>, tells <code>cargo</code> to not use the default
rust benchmarking harness which is important because Iai-Callgrind has an own
benchmarking harness.</p>
<p>Then copy the following content into this file:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::{main, library_benchmark_group, library_benchmark};
use std::hint::black_box;

fn fibonacci(n: u64) -&gt; u64 {
    match n {
        0 =&gt; 1,
        1 =&gt; 1,
        n =&gt; fibonacci(n - 1) + fibonacci(n - 2),
    }
}

#[library_benchmark]
#[bench::short(10)]
#[bench::long(30)]
fn bench_fibonacci(value: u64) -&gt; u64 {
    black_box(fibonacci(value))
}

library_benchmark_group!(
    name = bench_fibonacci_group;
    benchmarks = bench_fibonacci
);

<span class="boring">fn main() {
</span>main!(library_benchmark_groups = bench_fibonacci_group);
<span class="boring">}</span></code></pre></pre>
<p>Now, that your first library benchmark is set up, you can run it with</p>
<pre><code class="language-shell">cargo bench
</code></pre>
<p>and should see something like the below</p>
<pre><code class="hljs"><span style="color:#0A0">library_benchmark::bench_fibonacci_group::bench_fibonacci</span> <span style="color:#0AA">short</span><span style="color:#0AA">:</span><b><span style="color:#00A">10</span></b>
  Instructions:     <b>           1734</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>           2359</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              0</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              3</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>           2362</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>           2464</b>|N/A             (<span style="color:#555">*********</span>)
<span style="color:#0A0">library_benchmark::bench_fibonacci_group::bench_fibonacci</span> <span style="color:#0AA">long</span><span style="color:#0AA">:</span><b><span style="color:#00A">30</span></b>
  Instructions:     <b>       26214734</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>       35638616</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              2</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              4</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>       35638622</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>       35638766</b>|N/A             (<span style="color:#555">*********</span>)</code></pre>
<p>In addition, you'll find the callgrind output and the output of other valgrind
tools in <code>target/iai</code>, if you want to investigate further with a tool like
<code>callgrind_annotate</code> etc.</p>
<p>When running the same benchmark again, the output will report the differences
between the current and the previous run. Say you've made change to the
<code>fibonacci</code> function, then you may see something like this:</p>
<pre><code class="hljs"><span style="color:#0A0">library_benchmark::bench_fibonacci_group::bench_fibonacci</span> <span style="color:#0AA">short</span><span style="color:#0AA">:</span><b><span style="color:#00A">10</span></b>
  Instructions:     <b>           2805</b>|1734            (<b><span style="color:#F55">+61.7647%</span></b>) [<b><span style="color:#F55">+1.61765x</span></b>]
  L1 Hits:          <b>           3815</b>|2359            (<b><span style="color:#F55">+61.7211%</span></b>) [<b><span style="color:#F55">+1.61721x</span></b>]
  L2 Hits:          <b>              0</b>|0               (<span style="color:#555">No change</span>)
  RAM Hits:         <b>              3</b>|3               (<span style="color:#555">No change</span>)
  Total read+write: <b>           3818</b>|2362            (<b><span style="color:#F55">+61.6427%</span></b>) [<b><span style="color:#F55">+1.61643x</span></b>]
  Estimated Cycles: <b>           3920</b>|2464            (<b><span style="color:#F55">+59.0909%</span></b>) [<b><span style="color:#F55">+1.59091x</span></b>]
<span style="color:#0A0">library_benchmark::bench_fibonacci_group::bench_fibonacci</span> <span style="color:#0AA">long</span><span style="color:#0AA">:</span><b><span style="color:#00A">30</span></b>
  Instructions:     <b>       16201597</b>|26214734        (<b><span style="color:#42c142">-38.1966%</span></b>) [<b><span style="color:#42c142">-1.61803x</span></b>]
  L1 Hits:          <b>       22025876</b>|35638616        (<b><span style="color:#42c142">-38.1966%</span></b>) [<b><span style="color:#42c142">-1.61803x</span></b>]
  L2 Hits:          <b>              2</b>|2               (<span style="color:#555">No change</span>)
  RAM Hits:         <b>              4</b>|4               (<span style="color:#555">No change</span>)
  Total read+write: <b>       22025882</b>|35638622        (<b><span style="color:#42c142">-38.1966%</span></b>) [<b><span style="color:#42c142">-1.61803x</span></b>]
  Estimated Cycles: <b>       22026026</b>|35638766        (<b><span style="color:#42c142">-38.1964%</span></b>) [<b><span style="color:#42c142">-1.61803x</span></b>]</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="anatomy-of-a-library-benchmark"><a class="header" href="#anatomy-of-a-library-benchmark">Anatomy of a library benchmark</a></h1>
<p>We're reusing our example from the <a href="benchmarks/library_benchmarks/./quickstart.html">Quickstart</a> section.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::{main, library_benchmark_group, library_benchmark};
use std::hint::black_box;

fn fibonacci(n: u64) -&gt; u64 {
    match n {
        0 =&gt; 1,
        1 =&gt; 1,
        n =&gt; fibonacci(n - 1) + fibonacci(n - 2),
    }
}

#[library_benchmark]
#[bench::short(10)]
#[bench::long(30)]
fn bench_fibonacci(value: u64) -&gt; u64 {
    black_box(fibonacci(value))
}

library_benchmark_group!(
    name = bench_fibonacci_group;
    benchmarks = bench_fibonacci
);

<span class="boring">fn main() {
</span>main!(library_benchmark_groups = bench_fibonacci_group);
<span class="boring">}</span></code></pre></pre>
<p>First of all, you need a public function in your library which you want to
benchmark. In this example this is the <code>fibonacci</code> function which, for the sake
of simplicity, lives in the benchmark file itself but doesn't have to. If it
would have been located in <code>my_lib::fibonacci</code>, you simply import that function
with <code>use my_lib::fibonacci</code> and go on as shown above. Next, you need a
<code>library_benchmark_group!</code> in which you specify the names of the benchmark
functions. Finally, the benchmark harness is created by the <code>main!</code> macro.</p>
<h2 id="the-benchmark-function"><a class="header" href="#the-benchmark-function">The benchmark function</a></h2>
<p>The benchmark function has to be annotated with the
<a href="benchmarks/library_benchmarks/./macros.html"><code>#[library_benchmark]</code></a> attribute. The
<a href="benchmarks/library_benchmarks/./macros.html"><code>#[bench]</code></a> attribute is an inner attribute of the
<code>#[library_benchmark]</code> attribute. It consists of a mandatory id (the <code>ID</code> part
in <code>#[bench::ID(/* ... */)]</code>) and in its most basic form, an optional list of
arguments which are passed to the benchmark function as parameters. Naturally,
the parameters of the benchmark function must match the argument list of the
<code>#[bench]</code> attribute. It is always a good idea to return something from the
benchmark function, here it is the computed <code>u64</code> value from the <code>fibonacci</code>
function wrapped in a <code>black_box</code>. See the docs of
<a href="https://doc.rust-lang.org/std/hint/fn.black_box.html"><code>std::hint::black_box</code></a>
for more information about its usage. Simply put, <em>all</em> values and variables in
the benchmarking function (but not in your library function) need to be wrapped
in a <code>black_box</code> except for the input parameters (here <code>value</code>) because
Iai-Callgrind already does that. But, it is no error to <code>black_box</code> the <code>value</code>
again.</p>
<p>The <code>bench</code> attribute takes any expression which includes function calls. The
following would have worked too and is one way to avoid the costs of the setup
code being attributed to the benchmarked function.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::{main, library_benchmark_group, library_benchmark};
use std::hint::black_box;

fn some_setup_func(value: u64) -&gt; u64 {
    value + 10
}

fn fibonacci(n: u64) -&gt; u64 {
    match n {
        0 =&gt; 1,
        1 =&gt; 1,
        n =&gt; fibonacci(n - 1) + fibonacci(n - 2),
    }
}

#[library_benchmark]
#[bench::short(10)]
// Note the usage of the `some_setup_func` in the argument list of this #[bench]
#[bench::long(some_setup_func(20))]
fn bench_fibonacci(value: u64) -&gt; u64 {
    black_box(fibonacci(value))
}

library_benchmark_group!(
   name = bench_fibonacci_group;
   benchmarks = bench_fibonacci
);

<span class="boring">fn main() {
</span>main!(library_benchmark_groups = bench_fibonacci_group);
<span class="boring">}</span></code></pre></pre>
<p>The perhaps most crucial part in setting up library benchmarks is to keep the
body of benchmark functions clean from any setup or teardown code. There are
also other ways to avoid setup and teardown code in the benchmark function,
which are discussed in full detail in the <a href="benchmarks/library_benchmarks/./setup_and_teardown.html">setup and
teardown</a> section.</p>
<h2 id="the-group"><a class="header" href="#the-group">The group</a></h2>
<p>The name of the benchmark functions, here the only benchmark function
<code>bench_fibonacci</code>, which should be benchmarked need to be specified in a
<code>library_benchmark_group!</code> in the <code>benchmarks</code> parameter. You can create as many
groups as you like and you can use it to organize related benchmarks. Each group
needs a unique <code>name</code>.</p>
<h2 id="the-main-macro"><a class="header" href="#the-main-macro">The main macro</a></h2>
<p>Each group you want to be benchmarked needs to be specified in the
<code>library_benchmark_groups</code> parameter of the <code>main!</code> macro and you're all set.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-macros-in-more-detail"><a class="header" href="#the-macros-in-more-detail">The macros in more detail</a></h1>
<p>This section is a brief reference to all the macros available in library
benchmarks. Feel free to come back here from other sections if you need a
reference. For the complete documentation of each macro see the <a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/">api
Documentation</a>.</p>
<p>For the following examples it is assumed that there is a file <code>lib.rs</code> in a
crate named <code>my_lib</code> with the following content:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span>pub fn bubble_sort(mut array: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    for i in 0..array.len() {
        for j in 0..array.len() - i - 1 {
            if array[j + 1] &lt; array[j] {
                array.swap(j, j + 1);
            }
        }
    }
    array
}
<span class="boring">}</span></code></pre></pre>
<h2 id="the-library_benchmark-attribute"><a class="header" href="#the-library_benchmark-attribute">The <code>#[library_benchmark]</code> attribute</a></h2>
<p>This attribute needs to be present on all benchmark functions specified in the
<a href="benchmarks/library_benchmarks/macros.html#the-library_benchmark_group-macro"><code>library_benchmark_group</code></a>. The benchmark
function can then be further annotated with the inner
<a href="benchmarks/library_benchmarks/macros.html#the-bench-attribute"><code>#[bench]</code></a> or <a href="benchmarks/library_benchmarks/macros.html#the-benches-attribute"><code>#[benches]</code></a>
attributes.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(value: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { value } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};
use std::hint::black_box;

#[library_benchmark]
#[bench::one(vec![1])]
#[benches::multiple(vec![1, 2], vec![1, 2, 3], vec![1, 2, 3, 4])]
fn bench_bubble_sort(values: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(values))
}

library_benchmark_group!(name = bubble_sort_group; benchmarks = bench_bubble_sort);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = bubble_sort_group);
<span class="boring">}</span></code></pre></pre>
<p>The following parameters are accepted:</p>
<ul>
<li><code>config</code>: Takes a
<a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/struct.LibraryBenchmarkConfig.html"><code>LibraryBenchmarkConfig</code></a></li>
<li><code>setup</code>: A global setup function which is applied to all following <a href="benchmarks/library_benchmarks/macros.html#the-bench-attribute"><code>#[bench]</code></a>
and <a href="benchmarks/library_benchmarks/macros.html#the-benches-attribute"><code>#[benches]</code></a> attributes if not overwritten by a <code>setup</code> parameter of these
attributes.</li>
<li><code>teardown</code>: Similar to <code>setup</code> but takes a global <code>teardown</code> function.</li>
</ul>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(value: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { value } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main, LibraryBenchmarkConfig};
use std::hint::black_box;

#[library_benchmark(
    config = LibraryBenchmarkConfig::default().truncate_description(None)
)]
#[bench::one(vec![1])]
fn bench_bubble_sort(values: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(values))
}

library_benchmark_group!(name = bubble_sort_group; benchmarks = bench_bubble_sort);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = bubble_sort_group);
<span class="boring">}</span></code></pre></pre>
<h3 id="the-bench-attribute"><a class="header" href="#the-bench-attribute">The <code>#[bench]</code> attribute</a></h3>
<p>The basic structure is <code>#[bench::some_id(/* parameters */)]</code>. The part after the
<code>::</code> must be an id unique within the same <code>#[library_benchmark]</code>. This attribute
accepts the following parameters:</p>
<ul>
<li><code>args</code>: A tuple with a list of arguments which are passed to the
benchmark function. The parentheses also need to be present if there is only a
single argument (<code>#[bench::my_id(args = (10))]</code>).</li>
<li><code>config</code>: Accepts a
<a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/struct.LibraryBenchmarkConfig.html"><code>LibraryBenchmarkConfig</code></a></li>
<li><code>setup</code>: A function which takes the arguments specified in the <code>args</code>
parameter and passes its return value to the benchmark function.</li>
<li><code>teardown</code>: A function which takes the return value of the benchmark function.</li>
</ul>
<p>If no other parameters besides <code>args</code> are present you can simply pass the
arguments as a list of values. So, instead of <code>#[bench::my_id(args = (10, 20))]</code>, you could also use the shorter <code>#[bench::my_id(10, 20)]</code>.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(value: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { value } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main, LibraryBenchmarkConfig};
use std::hint::black_box;

// This function is used to create a worst case array we want to sort with our implementation of
// bubble sort
pub fn worst_case(start: i32) -&gt; Vec&lt;i32&gt; {
    if start.is_negative() {
        (start..0).rev().collect()
    } else {
        (0..start).rev().collect()
    }
}

#[library_benchmark]
#[bench::one(vec![1])]
#[bench::worst_two(args = (vec![2, 1]))]
#[bench::worst_four(args = (4), setup = worst_case)]
fn bench_bubble_sort(value: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(value))
}

library_benchmark_group!(name = bubble_sort_group; benchmarks = bench_bubble_sort);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = bubble_sort_group);
<span class="boring">}</span></code></pre></pre>
<h3 id="the-benches-attribute"><a class="header" href="#the-benches-attribute">The <code>#[benches]</code> attribute</a></h3>
<p>This attribute is used to specify multiple benchmarks at once. It accepts the
same parameters as the <a href="benchmarks/library_benchmarks/macros.html#the-bench-attribute"><code>#[bench]</code></a> attribute: <code>args</code>,
<code>config</code>, <code>setup</code> and <code>teardown</code> and additionally the <code>file</code> parameter which is
explained in detail <a href="benchmarks/library_benchmarks/./multiple_benches.html">here</a>. In contrast to the <code>args</code>
parameter in <a href="benchmarks/library_benchmarks/macros.html#the-bench-attribute"><code>#[bench]</code></a>, <code>args</code> takes an array of
arguments.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(value: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { value } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main, LibraryBenchmarkConfig};
use std::hint::black_box;

pub fn worst_case(start: i32) -&gt; Vec&lt;i32&gt; {
    if start.is_negative() {
        (start..0).rev().collect()
    } else {
        (0..start).rev().collect()
    }
}

#[library_benchmark]
#[benches::worst_two_and_three(args = [vec![2, 1], vec![3, 2, 1]])]
#[benches::worst_four_to_nine(args = [4, 5, 6, 7, 8, 9], setup = worst_case)]
fn bench_bubble_sort(value: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(value))
}

library_benchmark_group!(name = bubble_sort_group; benchmarks = bench_bubble_sort);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = bubble_sort_group);
<span class="boring">}</span></code></pre></pre>
<h2 id="the-library_benchmark_group-macro"><a class="header" href="#the-library_benchmark_group-macro">The library_benchmark_group! macro</a></h2>
<p>The <code>library_benchmark_group</code> macro accepts the following parameters (in this
order and separated by a semicolon):</p>
<ul>
<li><strong><code>name</code></strong> (mandatory): A unique name used to identify the group for the
<code>main!</code> macro</li>
<li><strong><code>config</code></strong> (optional): A
<a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/struct.LibraryBenchmarkConfig.html"><code>LibraryBenchmarkConfig</code></a>
which is applied to all benchmarks within the same group.</li>
<li><strong><code>compare_by_id</code></strong> (optional): The default is false. If true, all benches in
the benchmark functions specified in the <code>benchmarks</code> parameter are compared
with each other as long as the ids (the part after the <code>::</code> in
<code>#[bench::id(...)]</code>) match. See also <a href="benchmarks/library_benchmarks/./compare_by_id.html">Comparing benchmark
functions</a></li>
<li><strong><code>setup</code></strong> (optional): A setup function or any valid expression which is run
before all benchmarks of this group</li>
<li><strong><code>teardown</code></strong> (optional): A teardown function or any valid expression which
is run after all benchmarks of this group</li>
<li><strong><code>benchmarks</code></strong> (mandatory): A list of comma separated paths of benchmark
functions which are annotated with <code>#[library_benchmark]</code></li>
</ul>
<p>Note the <code>setup</code> and <code>teardown</code> parameters are different to the ones of
<code>#[library_benchmark]</code>, <code>#[bench]</code> and <code>#[benches]</code>. They accept an expression
or function call as in <code>setup = group_setup_function()</code>. Also, these <code>setup</code> and
<code>teardown</code> functions are not overridden by the ones from any of the before
mentioned attributes.</p>
<h2 id="the-main-macro-1"><a class="header" href="#the-main-macro-1">The main! macro</a></h2>
<p>This macro is the entry point for Iai-Callgrind and creates the benchmark
harness. It accepts the following top-level arguments in this order (separated
by a semicolon):</p>
<ul>
<li><strong><code>config</code></strong> (optional): Optionally specify a
<a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/struct.LibraryBenchmarkConfig.html"><code>LibraryBenchmarkConfig</code></a></li>
<li><strong><code>setup</code></strong> (optional): A setup function or any valid expression which is run
before all benchmarks</li>
<li><strong><code>teardown</code></strong> (optional): A setup function or any valid expression which is
run after all benchmarks</li>
<li><strong><code>library_benchmark_groups</code></strong> (mandatory): The name of one or more library
benchmark groups. Multiple names are separated by a comma.</li>
</ul>
<p>Like the <code>setup</code> and <code>teardown</code> of the
<a href="benchmarks/library_benchmarks/macros.html#the-library_benchmark_group-macro"><code>library_benchmark_group</code></a>, these
parameters accept an expression and are not overridden by the <code>setup</code> and
<code>teardown</code> of the <code>library_benchmark_group</code>, <code>#[library_benchmark]</code>, <code>#[bench]</code>
or <code>#[benches]</code> attribute.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable MD041 MD033 -->
<h1 id="setup-and-teardown"><a class="header" href="#setup-and-teardown">setup and teardown</a></h1>
<p><code>setup</code> and <code>teardown</code> are your bread and butter in library benchmarks. The
benchmark functions need to be as clean as possible and almost always only
contain the function call to the function of your library which you want to
benchmark.</p>
<h2 id="setup"><a class="header" href="#setup">Setup</a></h2>
<p>In an ideal world you don't need any setup code and you can pass arguments to
the function as they are.</p>
<p>But, for example if a function expects a <code>File</code> and not a <code>&amp;str</code> with the path
to the file you need <code>setup</code> code. Iai-Callgrind has an easy to use system in
place to allow you to run any setup code before the function is executed and
this <code>setup</code> code is not attributed to the metrics of the benchmark.</p>
<p>If the <code>setup</code> parameter is specified, the <code>setup</code> function takes the arguments
from the <code>#[bench]</code> (or <code>#[benches]</code>) attributes and the benchmark function
receives the return value of the <code>setup</code> function as parameter. This is a small
indirection with great effect and is easy to get used to quickly. The effect is
best shown with an example:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn count_bytes_fast(_file: std::fs::File) -&gt; u64 { 1 } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};

use std::hint::black_box;
use std::path::PathBuf;
use std::fs::File;

fn open_file(path: &amp;str) -&gt; File {
    File::open(path).unwrap()
}

#[library_benchmark]
#[bench::first(args = ("path/to/file"), setup = open_file)]
fn count_bytes_fast(file: File) -&gt; u64 {
    black_box(my_lib::count_bytes_fast(file))
}

library_benchmark_group!(name = my_group; benchmarks = count_bytes_fast);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>You can actually see the effect of using a setup function in the output of the
benchmark. Let's assume the above benchmark is in a file
<code>benches/my_benchmark.rs</code>, then running</p>
<pre><code class="language-shell">IAI_CALLGRIND_NOCAPTURE=true cargo bench
</code></pre>
<p>result in the benchmark output like below.</p>
<pre><code class="hljs"><span style="color:#0A0">my_benchmark::my_group::count_bytes_fast</span> <span style="color:#0AA">first</span><span style="color:#0AA">:</span><b><span style="color:#00A">open_file("path/to/file")</span></b>
  Instructions:     <b>        1630162</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>        2507933</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              2</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>             11</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>        2507946</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>        2508328</b>|N/A             (<span style="color:#555">*********</span>)</code></pre>
<p>The description in the headline contains <code>open_file("path/to/file")</code>, your setup
function <code>open_file</code> with the value of the parameter it is called with.</p>
<p>If you need to specify the same <code>setup</code> function for all (or almost all)
<code>#[bench]</code> and <code>#[benches]</code> in a <code>#[library_benchmark]</code> you can use the <code>setup</code>
parameter of the <code>#[library_benchmark]</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn count_bytes_fast(_file: std::fs::File) -&gt; u64 { 1 } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};

use std::hint::black_box;
use std::path::PathBuf;
use std::fs::File;
use std::io::{Seek, SeekFrom};

fn open_file(path: &amp;str) -&gt; File {
    File::open(path).unwrap()
}

fn open_file_with_offset(path: &amp;str, offset: u64) -&gt; File {
    let mut file = File::open(path).unwrap();
    file.seek(SeekFrom::Start(offset)).unwrap();
    file
}

#[library_benchmark(setup = open_file)]
#[bench::small("path/to/small")]
#[bench::big("path/to/big")]
#[bench::with_offset(args = ("path/to/big", 100), setup = open_file_with_offset)]
fn count_bytes_fast(file: File) -&gt; u64 {
    black_box(my_lib::count_bytes_fast(file))
}

library_benchmark_group!(name = my_group; benchmarks = count_bytes_fast);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>The above will use the <code>open_file</code> function in the <code>small</code> and <code>big</code> benchmarks
and the <code>open_file_with_offset</code> function in the <code>with_offset</code> benchmark.</p>
<h2 id="teardown"><a class="header" href="#teardown">Teardown</a></h2>
<p>What about <code>teardown</code> and why should you use it? Usually the <code>teardown</code> isn't
needed but for example if you intend to make the result from the benchmark
visible in the benchmark output, the <code>teardown</code> is the perfect place to do so.</p>
<p>The <code>teardown</code> function takes the return value of the benchmark function as its
argument:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn count_bytes_fast(_file: std::fs::File) -&gt; u64 { 1 } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};

use std::hint::black_box;
use std::path::PathBuf;
use std::fs::File;

fn open_file(path: &amp;str) -&gt; File {
    File::open(path).unwrap()
}

fn print_bytes_read(num_bytes: u64) {
    println!("bytes read: {num_bytes}");
}

#[library_benchmark]
#[bench::first(
    args = ("path/to/big"),
    setup = open_file,
    teardown = print_bytes_read
)]
fn count_bytes_fast(file: File) -&gt; u64 {
    black_box(my_lib::count_bytes_fast(file))
}

library_benchmark_group!(name = my_group; benchmarks = count_bytes_fast);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>Note Iai-Callgrind captures all output per default. In order to actually see the
output of the benchmark, <code>setup</code> and <code>teardown</code> functions, it is required to run
the benchmarks with the flag <code>--nocapture</code> or set the environment variable
<code>IAI_CALLGRIND_NOCAPTURE=true</code>. Let's assume the above benchmark is in a file
<code>benches/my_benchmark.rs</code>, then running</p>
<pre><code class="language-shell">IAI_CALLGRIND_NOCAPTURE=true cargo bench
</code></pre>
<p>results in output like the below</p>
<pre><code class="hljs"><span style="color:#0A0">my_benchmark::my_group::count_bytes_fast</span> <span style="color:#0AA">first</span><span style="color:#0AA">:</span><b><span style="color:#00A">open_file("path/to/big")</span></b>
bytes read: 25078
<span style="color:#A50">-</span> <span style="color:#A50">end of stdout/stderr</span>
  Instructions:     <b>        1630162</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>        2507931</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              2</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>             13</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>        2507946</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>        2508396</b>|N/A             (<span style="color:#555">*********</span>)</code></pre>
<p>The output of the <code>teardown</code> function is now visible in the benchmark output
above the <code>- end of stdout/stderr</code> line.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="specifying-multiple-benches-at-once"><a class="header" href="#specifying-multiple-benches-at-once">Specifying multiple benches at once</a></h1>
<p>Multiple benches can be specified at once with the
<a href="benchmarks/library_benchmarks/macros.html#the-benches-attribute"><code>#[benches]</code></a> attribute.</p>
<h2 id="the-benches-attribute-in-more-detail"><a class="header" href="#the-benches-attribute-in-more-detail">The <code>#[benches]</code> attribute in more detail</a></h2>
<p>Let's start with an example:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(value: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { value } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};
use std::hint::black_box;
use my_lib::bubble_sort;

fn setup_worst_case_array(start: i32) -&gt; Vec&lt;i32&gt; {
    if start.is_negative() {
        (start..0).rev().collect()
    } else {
        (0..start).rev().collect()
    }
}

#[library_benchmark]
#[benches::multiple(vec![1], vec![5])]
#[benches::with_setup(args = [1, 5], setup = setup_worst_case_array)]
fn bench_bubble_sort_with_benches_attribute(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(bubble_sort(input))
}

library_benchmark_group!(name = my_group; benchmarks = bench_bubble_sort_with_benches_attribute);
<span class="boring">fn main () {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>Usually the <code>arguments</code> are passed directly to the benchmarking function as it
can be seen in the <code>#[benches::multiple(/* arguments */)]</code> case. In
<code>#[benches::with_setup(/* ... */)]</code>, the arguments are passed to the <code>setup</code> function
instead. The above <code>#[library_benchmark]</code> is pretty much the same as</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(value: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { value } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};
use std::hint::black_box;
use my_lib::bubble_sort;

fn setup_worst_case_array(start: i32) -&gt; Vec&lt;i32&gt; {
    if start.is_negative() {
        (start..0).rev().collect()
    } else {
        (0..start).rev().collect()
    }
}

#[library_benchmark]
#[bench::multiple_0(vec![1])]
#[bench::multiple_1(vec![5])]
#[bench::with_setup_0(setup_worst_case_array(1))]
#[bench::with_setup_1(setup_worst_case_array(5))]
fn bench_bubble_sort_with_benches_attribute(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(bubble_sort(input))
}

library_benchmark_group!(name = my_group; benchmarks = bench_bubble_sort_with_benches_attribute);
<span class="boring">fn main () {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>but a lot more concise especially if a lot of values are passed to the same
<code>setup</code> function.</p>
<h3 id="the-file-parameter"><a class="header" href="#the-file-parameter">The <code>file</code> parameter</a></h3>
<p>Reading inputs from a file allows for example sharing the same inputs between
different benchmarking frameworks like <code>criterion</code> or if you simply have a long
list of inputs you might find it more convenient to read them from a file.</p>
<p>The <code>file</code> parameter, exclusive to the <code>#[benches]</code> attribute, does exactly that
and reads the specified file line by line creating a benchmark from each line.
The line is passed to the benchmark function as <code>String</code> or if the <code>setup</code>
parameter is also present to the <code>setup</code> function. A small example assuming you
have a file <code>benches/inputs</code> (relative paths are interpreted to the workspace
root) with the following content</p>
<pre><code class="language-text">1
11
111
</code></pre>
<p>then</p>
<pre><code class="language-rust ignore"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn string_to_u64(value: String) -&gt; Result&lt;u64, String&gt; { Ok(1) } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};
use std::hint::black_box;

#[library_benchmark]
#[benches::from_file(file = "benches/inputs")]
fn some_bench(line: String) -&gt; Result&lt;u64, String&gt; {
    black_box(my_lib::string_to_u64(line))
}

library_benchmark_group!(name = my_group; benchmarks = some_bench);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre>
<p>The above is roughly equivalent to the following but with the <code>args</code> parameter</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn string_to_u64(value: String) -&gt; Result&lt;u64, String&gt; { Ok(1) } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};
use std::hint::black_box;

#[library_benchmark]
#[benches::from_args(args = [1.to_string(), 11.to_string(), 111.to_string()])]
fn some_bench(line: String) -&gt; Result&lt;u64, String&gt; {
    black_box(my_lib::string_to_u64(line))
}

library_benchmark_group!(name = my_group; benchmarks = some_bench);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>The true power of the <code>file</code> parameter comes with the <code>setup</code> function because
you can format the lines in the file as you like and convert each line in the
<code>setup</code> function to the format as you need it in the benchmark. For example if
you decided to go with a csv like format in the file <code>benches/inputs</code></p>
<pre><code class="language-text">255;255;255
0;0;0
</code></pre>
<p>and your library has a function which converts from RGB to HSV color space:</p>
<pre><code class="language-rust ignore"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn rgb_to_hsv(a: u8, b: u8, c:u8) -&gt; (u16, u8, u8) { (a.into(), b, c) } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};
use std::hint::black_box;

fn decode_line(line: String) -&gt; (u8, u8, u8) {
    if let &amp;[a, b, c] = line.split(";")
        .map(|s| s.parse::&lt;u8&gt;().unwrap())
        .collect::&lt;Vec&lt;u8&gt;&gt;()
        .as_slice() 
    {
        (a, b, c)
    } else {
        panic!("Wrong input format in line '{line}'");
    }
}

#[library_benchmark]
#[benches::from_file(file = "benches/inputs", setup = decode_line)]
fn some_bench((a, b, c): (u8, u8, u8)) -&gt; (u16, u8, u8) {
    black_box(my_lib::rgb_to_hsv(black_box(a), black_box(b), black_box(c)))
}

library_benchmark_group!(name = my_group; benchmarks = some_bench);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="generic-benchmark-functions"><a class="header" href="#generic-benchmark-functions">Generic benchmark functions</a></h1>
<p>Benchmark functions can be generic. And <code>setup</code> and <code>teardown</code> functions, too.
There's actually not much more to say about it since generic benchmark (<code>setup</code>
and <code>teardown</code>) functions behave exactly the same way as you would expect it
from any other generic function.</p>
<p>However, there is a common pitfall. If you have a function
<code>count_lines_in_file_fast</code> which expects as parameter a <code>PathBuf</code> and although
it is convenient especially when you have to specify many paths, don't do this:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn count_lines_in_file_fast(_path: std::path::PathBuf) -&gt; u64 { 1 } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};

use std::hint::black_box;
use std::path::PathBuf;

#[library_benchmark]
#[bench::first("path/to/file")]
fn generic_bench&lt;T&gt;(path: T) -&gt; u64 where T: Into&lt;PathBuf&gt; {
    black_box(my_lib::count_lines_in_file_fast(black_box(path.into())))
}

library_benchmark_group!(name = my_group; benchmarks = generic_bench);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>Since <code>path.into()</code> is called in the benchmark function itself, the conversion
from a <code>&amp;str</code> to a <code>PathBuf</code> is attributed to the benchmark metrics. This is
almost never what you intended. You should instead convert the argument to a
<code>PathBuf</code> in a generic <code>setup</code> function like that:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn count_lines_in_file_fast(_path: std::path::PathBuf) -&gt; u64 { 1 } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};

use std::hint::black_box;
use std::path::PathBuf;

fn convert_to_pathbuf&lt;T&gt;(path: T) -&gt; PathBuf where T: Into&lt;PathBuf&gt; {
    path.into()
}

#[library_benchmark]
#[bench::first(args = ("path/to/file"), setup = convert_to_pathbuf)]
fn not_generic_anymore(path: PathBuf) -&gt; u64 {
    black_box(my_lib::count_lines_in_file_fast(path))
}

library_benchmark_group!(name = my_group; benchmarks = not_generic_anymore);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>That way you can still enjoy the convenience to use string literals instead of
<code>PathBuf</code> in your <code>#[bench]</code> (or <code>#[benches]</code>) arguments and have clean
benchmark metrics.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable MD025 MD042 MD033 -->
<h1 id="comparing-benchmark-functions"><a class="header" href="#comparing-benchmark-functions">Comparing benchmark functions</a></h1>
<p>Comparing benchmark functions is supported via the optional
<code>library_benchmark_group!</code> argument <code>compare_by_id</code> (The default value for
<code>compare_by_id</code> is <code>false</code>). Only benches with the same <code>id</code> are compared, which
allows to single out cases which don't need to be compared. In the following
example, the <code>case_3</code> and <code>multiple</code> bench are compared with each other in
addition to the usual comparison with the previous run:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(_: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec![] } }
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};
use std::hint::black_box;

#[library_benchmark]
#[bench::case_3(vec![1, 2, 3])]
#[benches::multiple(args = [vec![1, 2], vec![1, 2, 3, 4]])]
fn bench_bubble_sort_best_case(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(input))
}

#[library_benchmark]
#[bench::case_3(vec![3, 2, 1])]
#[benches::multiple(args = [vec![2, 1], vec![4, 3, 2, 1]])]
fn bench_bubble_sort_worst_case(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(input))
}

library_benchmark_group!(
    name = bench_bubble_sort;
    compare_by_id = true;
    benchmarks = bench_bubble_sort_best_case, bench_bubble_sort_worst_case
);

<span class="boring">fn main() {
</span>main!(library_benchmark_groups = bench_bubble_sort);
<span class="boring">}</span></code></pre></pre>
<p>Note if <code>compare_by_id</code> is <code>true</code>, all benchmark functions are compared with
each other, so you are not limited to two benchmark functions per comparison
group.</p>
<p>Here's a the benchmark output of the above example to see what is happening:</p>
<pre><code class="hljs"><span style="color:#0A0">my_benchmark::bubble_sort_group::bubble_sort_best_case</span> <span style="color:#0AA">case_2</span><span style="color:#0AA">:</span><b><span style="color:#00A">vec! [1, 2]</span></b>
  Instructions:     <b>             63</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>             86</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              1</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              4</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>             91</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>            231</b>|N/A             (<span style="color:#555">*********</span>)
<span style="color:#0A0">my_benchmark::bubble_sort_group::bubble_sort_best_case</span> <span style="color:#0AA">multiple_0</span><span style="color:#0AA">:</span><b><span style="color:#00A">vec! [1, 2, 3]</span></b>
  Instructions:     <b>             94</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>            123</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              1</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              4</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>            128</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>            268</b>|N/A             (<span style="color:#555">*********</span>)
<span style="color:#0A0">my_benchmark::bubble_sort_group::bubble_sort_best_case</span> <span style="color:#0AA">multiple_1</span><span style="color:#0AA">:</span><b><span style="color:#00A">vec! [1, 2, 3, 4]</span></b>
  Instructions:     <b>            136</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>            174</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              1</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              4</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>            179</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>            319</b>|N/A             (<span style="color:#555">*********</span>)
<span style="color:#0A0">my_benchmark::bubble_sort_group::bubble_sort_worst_case</span> <span style="color:#0AA">case_2</span><span style="color:#0AA">:</span><b><span style="color:#00A">vec! [2, 1]</span></b>
  Instructions:     <b>             66</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>             91</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              1</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              4</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>             96</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>            236</b>|N/A             (<span style="color:#555">*********</span>)
  <b><span style="color:#A50">Comparison with</span></b> <span style="color:#0A0">bubble_sort_best_case</span> <span style="color:#0AA">case_2</span>:<b><span style="color:#00A">vec! [1, 2]</span></b>
  Instructions:     <b>             63</b>|66              (<b><span style="color:#42c142">-4.54545%</span></b>) [<b><span style="color:#42c142">-1.04762x</span></b>]
  L1 Hits:          <b>             86</b>|91              (<b><span style="color:#42c142">-5.49451%</span></b>) [<b><span style="color:#42c142">-1.05814x</span></b>]
  L2 Hits:          <b>              1</b>|1               (<span style="color:#555">No change</span>)
  RAM Hits:         <b>              4</b>|4               (<span style="color:#555">No change</span>)
  Total read+write: <b>             91</b>|96              (<b><span style="color:#42c142">-5.20833%</span></b>) [<b><span style="color:#42c142">-1.05495x</span></b>]
  Estimated Cycles: <b>            231</b>|236             (<b><span style="color:#42c142">-2.11864%</span></b>) [<b><span style="color:#42c142">-1.02165x</span></b>]
<span style="color:#0A0">my_benchmark::bubble_sort_group::bubble_sort_worst_case</span> <span style="color:#0AA">multiple_0</span><span style="color:#0AA">:</span><b><span style="color:#00A">vec! [3, 2, 1]</span></b>
  Instructions:     <b>            103</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>            138</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              1</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              4</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>            143</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>            283</b>|N/A             (<span style="color:#555">*********</span>)
  <b><span style="color:#A50">Comparison with</span></b> <span style="color:#0A0">bubble_sort_best_case</span> <span style="color:#0AA">multiple_0</span>:<b><span style="color:#00A">vec! [1, 2, 3]</span></b>
  Instructions:     <b>             94</b>|103             (<b><span style="color:#42c142">-8.73786%</span></b>) [<b><span style="color:#42c142">-1.09574x</span></b>]
  L1 Hits:          <b>            123</b>|138             (<b><span style="color:#42c142">-10.8696%</span></b>) [<b><span style="color:#42c142">-1.12195x</span></b>]
  L2 Hits:          <b>              1</b>|1               (<span style="color:#555">No change</span>)
  RAM Hits:         <b>              4</b>|4               (<span style="color:#555">No change</span>)
  Total read+write: <b>            128</b>|143             (<b><span style="color:#42c142">-10.4895%</span></b>) [<b><span style="color:#42c142">-1.11719x</span></b>]
  Estimated Cycles: <b>            268</b>|283             (<b><span style="color:#42c142">-5.30035%</span></b>) [<b><span style="color:#42c142">-1.05597x</span></b>]
<span style="color:#0A0">my_benchmark::bubble_sort_group::bubble_sort_worst_case</span> <span style="color:#0AA">multiple_1</span><span style="color:#0AA">:</span><b><span style="color:#00A">vec! [4, 3, 2, 1]</span></b>
  Instructions:     <b>            154</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>            204</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              1</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              4</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>            209</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>            349</b>|N/A             (<span style="color:#555">*********</span>)
  <b><span style="color:#A50">Comparison with</span></b> <span style="color:#0A0">bubble_sort_best_case</span> <span style="color:#0AA">multiple_1</span>:<b><span style="color:#00A">vec! [1, 2, 3, 4]</span></b>
  Instructions:     <b>            136</b>|154             (<b><span style="color:#42c142">-11.6883%</span></b>) [<b><span style="color:#42c142">-1.13235x</span></b>]
  L1 Hits:          <b>            174</b>|204             (<b><span style="color:#42c142">-14.7059%</span></b>) [<b><span style="color:#42c142">-1.17241x</span></b>]
  L2 Hits:          <b>              1</b>|1               (<span style="color:#555">No change</span>)
  RAM Hits:         <b>              4</b>|4               (<span style="color:#555">No change</span>)
  Total read+write: <b>            179</b>|209             (<b><span style="color:#42c142">-14.3541%</span></b>) [<b><span style="color:#42c142">-1.16760x</span></b>]
  Estimated Cycles: <b>            319</b>|349             (<b><span style="color:#42c142">-8.59599%</span></b>) [<b><span style="color:#42c142">-1.09404x</span></b>]</code></pre>
<p>The procedure of the comparison algorithm:</p>
<ol>
<li>Run all benches in the first benchmark function</li>
<li>Run the first bench in the second benchmark function and if there is a bench
in the first benchmark function with the same id compare them</li>
<li>Run the second bench in the second benchmark function ...</li>
<li>...</li>
<li>Run the first bench in the third benchmark function and if there is a bench
in the first benchmark function with the same id compare them. If there is a
bench with the same id in the second benchmark function compare them.</li>
<li>Run the second bench in the third benchmark function ...</li>
<li>and so on ... until all benches are compared with each other</li>
</ol>
<p>Neither the order nor the amount of benches within the benchmark functions
matters, so it is not strictly necessary to mirror the bench ids of the first
benchmark function in the second, third, etc. benchmark function.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration"><a class="header" href="#configuration">Configuration</a></h1>
<p>Library benchmarks can be configured with the <a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/struct.LibraryBenchmarkConfig.html"><code>LibraryBenchmarkConfig</code></a> and
with <a href="benchmarks/library_benchmarks/../../cli_and_env/basics.html">Command-line arguments and Environment
variables</a>.</p>
<p>The <code>LibraryBenchmarkConfig</code> can be specified at different levels and sets the
configuration values for the same and lower levels. The values of the
<code>LibraryBenchmarkConfig</code> at higher levels can be overridden at a lower level.
Note that some values are additive rather than substitutive. Please see the docs
of the respective functions in <a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/struct.LibraryBenchmarkConfig.html"><code>LibraryBenchmarkConfig</code></a> for more details.</p>
<p>The different levels where a <code>LibraryBenchmarkConfig</code> can be specified.</p>
<ul>
<li>At top-level with the <code>main!</code> macro</li>
</ul>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">use iai_callgrind::{library_benchmark, library_benchmark_group};
</span>use iai_callgrind::{main, LibraryBenchmarkConfig};

<span class="boring">#[library_benchmark] fn bench() {}
</span><span class="boring">library_benchmark_group!(name = my_group; benchmarks = bench);
</span><span class="boring">fn main() {
</span>main!(
    config = LibraryBenchmarkConfig::default();
    library_benchmark_groups = my_group
);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>At group-level in the <code>library_benchmark_group!</code> macro</li>
</ul>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">use iai_callgrind::library_benchmark;
</span>use iai_callgrind::{main, LibraryBenchmarkConfig, library_benchmark_group};

<span class="boring">#[library_benchmark] fn bench() {}
</span>library_benchmark_group!(
    name = my_group;
    config = LibraryBenchmarkConfig::default();
    benchmarks = bench
);

<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>At <code>#[library_benchmark]</code> level</li>
</ul>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(_: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec![] } }
</span>use iai_callgrind::{
    main, LibraryBenchmarkConfig, library_benchmark_group, library_benchmark
};
use std::hint::black_box;

#[library_benchmark(config = LibraryBenchmarkConfig::default())] 
fn bench() {
    /* ... */
}

library_benchmark_group!(
    name = my_group;
    config = LibraryBenchmarkConfig::default();
    benchmarks = bench
);

<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<ul>
<li>and at <code>#[bench]</code>, <code>#[benches]</code> level</li>
</ul>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(_: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec![] } }
</span>use iai_callgrind::{
    main, LibraryBenchmarkConfig, library_benchmark_group, library_benchmark
};
use std::hint::black_box;

#[library_benchmark] 
#[bench::some_id(args = (1, 2), config = LibraryBenchmarkConfig::default())]
#[benches::multiple(
    args = [(3, 4), (5, 6)], 
    config = LibraryBenchmarkConfig::default()
)]
fn bench(a: u8, b: u8) {
    /* ... */
<span class="boring">    _ = (a, b);
</span>}

library_benchmark_group!(
    name = my_group;
    config = LibraryBenchmarkConfig::default();
    benchmarks = bench
);

<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="custom-entry-points"><a class="header" href="#custom-entry-points">Custom entry points</a></h1>
<p>To understand custom entry points let's take a small detour into how
<a href="https://valgrind.org/docs/manual/cl-manual.html"><code>Callgrind</code></a> and Iai-Callgrind work under the hood.</p>
<h2 id="iai-callgrind-under-the-hood"><a class="header" href="#iai-callgrind-under-the-hood">Iai-Callgrind under the hood</a></h2>
<p><code>Callgrind</code> collects metrics and associates them with a function. This happens
based on the compiled code not the source code, so it is possible to hook into
any function not only public functions. <code>Callgrind</code> can be configured to switch
instrumentation on and off based on a function name with
<a href="https://valgrind.org/docs/manual/cl-manual.html#cl-manual.options"><code>--toggle-collect</code></a>. Per default, Iai-Callgrind sets this
toggle (which we call <a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/enum.EntryPoint.html"><code>EntryPoint</code></a>) to the benchmarking function. Setting the
toggle implies <code>--collect-at-start=no</code>. So, all events before (in the <code>setup</code>)
and after the benchmark function (in the <code>teardown</code>) are not collected. Somewhat
simplified, but conveying the basic idea, here is a commented example:</p>
<pre><pre class="playground"><code class="language-rust edition2021">// &lt;-- collect-at-start=no

<span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(_: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec![] } }
</span>use iai_callgrind::{main,library_benchmark_group, library_benchmark};
use std::hint::black_box;

#[library_benchmark]
fn bench() -&gt; Vec&lt;i32&gt; { // &lt;-- DEFAULT ENTRY POINT starts collecting events
    black_box(my_lib::bubble_sort(vec![3, 2, 1]))
} // &lt;-- stop collecting events

library_benchmark_group!( name = my_group; benchmarks = bench);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<h3 id="pitfall-inlined-functions"><a class="header" href="#pitfall-inlined-functions">Pitfall: Inlined functions</a></h3>
<p>The fact that <code>Callgrind</code> acts on the compiled code harbors a pitfall. The
compiler with compile-time optimizations switched on (which is usually the case
when compiling benchmarks) inlines functions if it sees an advantage in doing
so. Iai-Callgrind takes care, that this doesn't happen with the benchmark
function, so <code>Callgrind</code> can find and hook into the benchmark function. But, in
your production code you actually don't want to stop the compiler from doing
it's job just to be able to benchmark that function. So, be cautious with
benchmarking private functions and only choose functions of which it is known
that they are not being inlined.</p>
<h2 id="hook-into-private-functions"><a class="header" href="#hook-into-private-functions">Hook into private functions</a></h2>
<p>The basic idea is to choose a public function in your library acting as access
point to the actual function you want to benchmark. As outlined before, this
works only reliably for functions which are not inlined by the compiler.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::{
    main, library_benchmark_group, library_benchmark, LibraryBenchmarkConfig,
    EntryPoint
};
use std::hint::black_box;

mod my_lib {
     #[inline(never)]
     fn bubble_sort(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
         // The algorithm
<span class="boring">       input
</span>     }

     pub fn access_point(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
         println!("Doing something before the function call");
         bubble_sort(input)
     }
}

#[library_benchmark(
    config = LibraryBenchmarkConfig::default()
        .entry_point(EntryPoint::Custom("*::my_lib::bubble_sort".to_owned()))
)]
#[bench::small(vec![3, 2, 1])]
#[bench::bigger(vec![5, 4, 3, 2, 1])]
fn bench_private(array: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::access_point(array))
}

library_benchmark_group!(name = my_group; benchmarks = bench_private);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>Note the <code>#[inline(never)]</code> we use in this example to make sure the
<code>bubble_sort</code> function is not getting inlined.</p>
<p>We use a <a href="https://valgrind.org/docs/manual/cl-manual.html#cl-manual.options">wildcard</a> <code>*::my_lib::bubble_sort</code> for
<code>EntryPoint::Custom</code> for demonstration purposes. You might want to tighten this
pattern. If you don't know how the pattern looks like, use <code>EntryPoint::None</code>
first then run the benchmark. Now, investigate the <a href="benchmarks/library_benchmarks/../../cli_and_env/output/out_directory.html">callgrind output
file</a>. This output file is pretty
low-level but all you need to do is search for the entries which start with
<code>fn=...</code>. In the example above this entry might look like
<code>fn=algorithms::my_lib::bubble_sort</code> if <code>my_lib</code> would be part of the top-level
<code>algorithms</code> module. Or, using grep:</p>
<pre><code class="language-shell">grep '^fn=.*::bubble_sort$' target/iai/the_package/benchmark_file_name/my_group/bench_private.bigger/callgrind.bench_private.bigger.out
</code></pre>
<p>Having found the pattern, you can eventually use <code>EntryPoint::Custom</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="even-more-examples"><a class="header" href="#even-more-examples">Even more Examples</a></h1>
<p>I'm referring here to the <a href="https://github.com/iai-callgrind/iai-callgrind">github
repository</a>. We test the library
benchmarks functionality of Iai-Callgrind with system tests in the private
<a href="https://github.com/iai-callgrind/iai-callgrind/tree/main/benchmark-tests/benches/test_lib_bench">benchmark-tests</a>
package.</p>
<p>Each system test there can serve you as an example, but for a fully documented
and commented one see
<a href="https://github.com/iai-callgrind/iai-callgrind/blob/main/benchmark-tests/benches/test_lib_bench/groups/test_lib_bench_groups.rs">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="binary-benchmarks"><a class="header" href="#binary-benchmarks">Binary Benchmarks</a></h1>
<p>You want to start benchmarking your crate's binary? You've come to the right
place. Best start with the <a href="benchmarks/./binary_benchmarks/quickstart.html">Quickstart</a>
section.</p>
<p>Setting up binary benchmarks is very similar to library benchmarks and it's a
good idea to have a look at the <a href="benchmarks/./library_benchmarks.html">library benchmark</a>
section of this guide, too.</p>
<p>Then come back to the binary benchmarks section and go through the
<a href="benchmarks/./binary_benchmarks/differences.html">differences</a></p>
<p>If you need more examples see <a href="benchmarks/./binary_benchmarks/examples.html">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="important-default-behaviour-1"><a class="header" href="#important-default-behaviour-1">Important default behaviour</a></h1>
<p>As in library benchmarks, the environment variables are cleared before running a
binary benchmark. Have a look at the <a href="benchmarks/binary_benchmarks/./configuration.html">Configuration</a> section
if you want to change this behavior.</p>
<p>Per default, the benchmarks run with cache simulation switched on. This adds
additional run time costs. If you don't need the cache metrics and estimation of
cycles, yan can easily switch cache simulation off with</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::BinaryBenchmarkConfig;

BinaryBenchmarkConfig::default().raw_callgrind_args(["--cache-sim=no"]);
<span class="boring">}</span></code></pre></pre>
<p>For example to switch off cache simulation for all benchmarks in the same file:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{
    binary_benchmark, binary_benchmark_group, main, BinaryBenchmarkConfig
};

#[binary_benchmark]
fn bench_binary() -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
}

binary_benchmark_group!(name = my_group; benchmarks = bench_binary);
<span class="boring">fn main() {
</span>main!(
    config = BinaryBenchmarkConfig::default().raw_callgrind_args(["--cache-sim=no"]);
    binary_benchmark_groups = my_group
);
<span class="boring">}</span></code></pre></pre>
<p>If you're new to Iai-Callgrind and don't know what the above means, don't panic.
Jump to <a href="benchmarks/binary_benchmarks/./quickstart.html">Quickstart</a> and read through the first few chapters,
and you're ready to benchmark with Iai-Callgrind like a pro.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable MD041 MD033 -->
<h1 id="quickstart-1"><a class="header" href="#quickstart-1">Quickstart</a></h1>
<p>Suppose the crate's binary is called <code>my-foo</code> and this binary takes a file path
as positional argument. This first example shows the basic usage of the
high-level api with the <code>#[binary_benchmark]</code> attribute:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{binary_benchmark, binary_benchmark_group, main};

#[binary_benchmark]
#[bench::some_id("foo.txt")]
fn bench_binary(path: &amp;str) -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
        .arg(path)
        .build()
}

binary_benchmark_group!(
    name = my_group;
    benchmarks = bench_binary
);

<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>If you want to try out this example with your crate's binary, put the above code
into a file in <code>$WORKSPACE_ROOT/benches/binary_benchmark.rs</code>. Next, replace
<code>my-foo</code> in <code>env!("CARGO_BIN_EXE_my-foo")</code> with the name of a binary of your
crate.</p>
<p>Note the <code>env!</code> macro is a <a href="https://doc.rust-lang.org/std/macro.env.html">rust</a>
builtin macro and <code>CARGO_BIN_EXE_&lt;name&gt;</code> is documented
<a href="https://doc.rust-lang.org/cargo/reference/environment-variables.html#environment-variables-cargo-sets-for-crates">here</a>.</p>
<p>You should always use <code>env!("CARGO_BIN_EXE_&lt;name&gt;")</code> to determine the path to
the binary of your crate. Do not use relative paths like <code>target/release/my-foo</code>
since this might break your benchmarks in many ways. The environment variable
does exactly the right thing and the usage is short and simple.</p>
<p>Lastly, adjust the argument of the <code>Command</code> and add the following to your
<code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[[bench]]
name = "binary_benchmark"
harness = false
</code></pre>
<p>Running</p>
<pre><code class="language-shell">cargo bench
</code></pre>
<p>presents you with something like the following:</p>
<pre><code class="hljs"><span style="color:#0A0">binary_benchmark::my_group::bench_binary</span> <span style="color:#0AA">some_id</span><span style="color:#0AA">:</span><b><span style="color:#00A">("foo.txt") -> target/release/my-foo foo.txt</span></b>
  Instructions:     <b>         342129</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>         457370</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>            734</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>           4096</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>         462200</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>         604400</b>|N/A             (<span style="color:#555">*********</span>)</code></pre>
<p>As opposed to library benchmarks, binary benchmarks have access to a <a href="benchmarks/binary_benchmarks/./low_level.html">low-level
api</a>. Here, pretty much the same as the above high-level usage
but written in the low-level api:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{BinaryBenchmark, Bench, binary_benchmark_group, main};

binary_benchmark_group!(
    name = my_group;
    benchmarks = |group: &amp;mut BinaryBenchmarkGroup| {
        group.binary_benchmark(BinaryBenchmark::new("bench_binary")
            .bench(Bench::new("some_id")
                .command(iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
                    .arg("foo.txt")
                    .build()
                )
            )
        )
    }
);

<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>If in doubt, use the high-level api. You can still
<a href="benchmarks/binary_benchmarks/./low_level.html#intermixing-high-level-and-low-level-api">migrate</a> to the
low-level api very easily if you really need to. The other way around is more
involved.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="differences-to-library-benchmarks"><a class="header" href="#differences-to-library-benchmarks">Differences to library benchmarks</a></h1>
<p>In this section we're going through the differences to <a href="benchmarks/binary_benchmarks/../library_benchmarks.html">library
benchmarks</a>. This assumes that you already know how to
set up library benchmarks and it is recommended to learn the very basics about
library benchmarks, starting with
<a href="benchmarks/binary_benchmarks/../binary_benchmarks/quickstart.html">Quickstart</a>, <a href="benchmarks/binary_benchmarks/../library_benchmarks/anatomy.html">Anatomy of a library
benchmark</a> and <a href="benchmarks/binary_benchmarks/../library_benchmarks/macros.html">The macros in more
detail</a>. Then come back to this section.</p>
<h2 id="name-changes"><a class="header" href="#name-changes">Name changes</a></h2>
<p>Coming from library benchmarks, the names with <code>library</code> in it change to the
same name but <code>library</code> with <code>binary</code> replaced, so the <code>#[library_benchmark]</code>
attribute's name changes to <code>#[binary_benchmark]</code> and <code>library_benchmark_group!</code>
changes to <code>binary_benchmark_group!</code>, the config arguments take a
<code>BinaryBenchmarkConfig</code> instead of a <code>LibraryBenchmarkConfig</code>...</p>
<p>A quick reference of available macros in binary benchmarks:</p>
<ul>
<li><code>#[binary_benchmark]</code> and its inner attributes <code>#[bench]</code> and <code>#[benches]</code>:
The exact pendant to the <code>#[library_benchmark]</code> attribute macro.</li>
<li><code>binary_benchmark_group!</code>: Just the name of the macro has changed.</li>
<li><code>binary_benchmark_attribute!</code>: An additional macro if you intend to
<a href="benchmarks/binary_benchmarks/./low_level.html#intermixing-high-level-and-low-level-api">migrate</a> from the high-level to the low-level
api</li>
<li><code>main!</code>: The same macro as in library benchmarks but the name of the
<code>library_benchmark_groups</code> parameter changed to <code>binary_benchmark_groups</code>.</li>
</ul>
<p>To see all macros in action have a look at the example below.</p>
<h2 id="the-return-value-of-the-benchmark-function"><a class="header" href="#the-return-value-of-the-benchmark-function">The return value of the benchmark function</a></h2>
<p>The maybe most important difference is, that the <code>#[binary_benchmark]</code> annotated
function always needs to return an <code>iai_callgrind::Command</code>. Note this function
builds the command which is going to be benchmarked but doesn't executed it,
yet. So, the code in this function does not attribute to the event counts of the
actual benchmark.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{binary_benchmark, binary_benchmark_group, main};
use std::path::PathBuf;

#[binary_benchmark]
#[bench::foo("foo.txt")]
#[bench::bar("bar.json")]
fn bench_binary(path: &amp;str) -&gt; iai_callgrind::Command {
    // We can put any code in this function which is needed to configure and
    // build the `Command`.
    let path = PathBuf::from(path);

    // Here, if the `path` ends with `.txt` we want to see
    // the `Stdout` output of the `Command` in the benchmark output. In all other 
    // cases, the `Stdout` of the `Command` is redirected to a `File` with the
    // same name as the input `path` but with the extension `out`.
    let stdout = if path.extension().unwrap() == "txt" {
        iai_callgrind::Stdio::Inherit
    } else {
        iai_callgrind::Stdio::File(path.with_extension("out"))
    };
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
        .stdout(stdout)
        .arg(path)
        .build()
}

binary_benchmark_group!(name = my_group; benchmarks = bench_binary);
<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<h2 id="setup-and-teardown-1"><a class="header" href="#setup-and-teardown-1"><code>setup</code> and <code>teardown</code></a></h2>
<p>Since we can put any code building the <code>Command</code> in the function itself, the
<code>setup</code> and <code>teardown</code> of <code>#[binary_benchmark]</code>, <code>#[bench]</code> and <code>#[benches]</code>
work differently.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{binary_benchmark, binary_benchmark_group, main};

fn create_file() {
    std::fs::write("foo.txt", "some content").unwrap();
}

#[binary_benchmark]
#[bench::foo(args = ("foo.txt"), setup = create_file())]
fn bench_binary(path: &amp;str) -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
        .arg(path)
        .build()
}

binary_benchmark_group!(name = my_group; benchmarks = bench_binary);
<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p><code>setup</code>, which is here the expression <code>create_file()</code>, is not evaluated right
away and the return value of <code>setup</code> is not used as input for the <code>function</code>!
Instead, the expression in <code>setup</code> is getting evaluated and executed just before
the benchmarked <code>Command</code> is <strong>executed</strong>. Similarly, <code>teardown</code> is executed
after the <code>Command</code> is <strong>executed</strong>.</p>
<p>In the example above, <code>setup</code> creates always the same file and is pretty static.
It's possible to use the same arguments for <code>setup</code> (<code>teardown</code>) <strong>and</strong> the
<code>function</code> using the path (or file pointer) to a function as you're used to from
library benchmarks:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{binary_benchmark, binary_benchmark_group, main};

fn create_file(path: &amp;str) {
    std::fs::write(path, "some content").unwrap();
}

fn delete_file(path: &amp;str) {
    std::fs::remove_file(path).unwrap();
}

#[binary_benchmark]
// Note the missing parentheses for `setup` of the function `create_file` which
// tells Iai-Callgrind to pass the `args` to the `setup` function AND the
// function `bench_binary`
#[bench::foo(args = ("foo.txt"), setup = create_file)]
// Same for `teardown`
#[bench::bar(args = ("bar.txt"), setup = create_file, teardown = delete_file)]
fn bench_binary(path: &amp;str) -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
        .arg(path)
        .build()
}

binary_benchmark_group!(name = my_group; benchmarks = bench_binary);
<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="the-commands-stdin-and-simulating-piped-input"><a class="header" href="#the-commands-stdin-and-simulating-piped-input">The Command's stdin and simulating piped input</a></h1>
<p>The behaviour of <code>Stdin</code> of the <code>Command</code> can be changed, almost the same way as
the <code>Stdin</code> of a <code>std::process::Command</code> with the only difference, that we use
the enums <code>iai_callgrind::Stdin</code> and <code>iai_callgrind::Stdio</code>. These enums provide
the variants <code>Inherit</code> (the equivalent of <code>std::process::Stdio::inherit</code>),
<code>Pipe</code> (the equivalent of <code>std::process::Stdio::piped</code>) and so on. There's also
<code>File</code> which takes a <code>PathBuf</code> to the file which is used as <code>Stdin</code> for the
<code>Command</code>. This corresponds to a redirection in the shell as in <code>my-foo &lt; path/to/file</code>.</p>
<p>Moreover, <code>iai_callgrind::Stdin</code> provides the <code>Stdin::Setup</code> variant specific to
Iai-Callgrind:</p>
<p>Applications may change their behaviour if the input or the <code>Stdin</code> of the
<code>Command</code> is coming from a pipe as in <code>echo "some content" | my-foo</code>. To be able
to benchmark such cases, it is possible to use the output of <code>setup</code> to <code>Stdout</code>
or <code>Stderr</code> as <code>Stdin</code> for the <code>Command</code>.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{binary_benchmark, binary_benchmark_group, main, Stdin, Pipe};

fn setup_pipe() {
    println!(
        "The output to `Stdout` here will be the input or `Stdin` of the `Command`"
    );
}

#[binary_benchmark]
#[bench::foo(setup = setup_pipe())]
fn bench_binary() -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
        .stdin(Stdin::Setup(Pipe::Stdout))
        .build()
}

binary_benchmark_group!(name = my_group; benchmarks = bench_binary);
<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>Usually, <code>setup</code> then the <code>Command</code> and then <code>teardown</code> are executed
sequentially, each waiting for the previous process to exit successfully (See
also <a href="benchmarks/binary_benchmarks/./configuration/exit_code.html">Configure the exit code of the Command</a>. If
the <code>Command::stdin</code> changes to <code>Stdin::Setup</code>, <code>setup</code> and the <code>Command</code> are
executed in parallel and Iai-Callgrind waits first for the <code>Command</code> to exit,
then <code>setup</code>. After the successful exit of <code>setup</code>, <code>teardown</code> is executed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration-1"><a class="header" href="#configuration-1">Configuration</a></h1>
<p>The configuration of binary benchmarks works the same way as in library
benchmarks with the name changing from <code>LibraryBenchmarkConfig</code> to
<code>BinaryBenchmarkConfig</code>. Please see
<a href="benchmarks/binary_benchmarks/../library_benchmarks/configuration.html">there</a> for the basics. However, Binary
benchmarks have some additional configuration possibilities:</p>
<ul>
<li><a href="benchmarks/binary_benchmarks/./configuration/sandbox.html">Sandbox</a></li>
<li><a href="benchmarks/binary_benchmarks/./configuration/exit_code.html">Configure the exit code of the Command</a>.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="sandbox"><a class="header" href="#sandbox">Sandbox</a></h1>
<p>The
<a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/struct.Sandbox.html"><code>Sandbox</code></a>
is a temporary directory which is created before the execution of the <code>setup</code>
and deleted after the <code>teardown</code>. <code>setup</code>, the <code>Command</code> and <code>teardown</code> are
executed inside this temporary directory. This simply describes the order of the
execution but the <code>setup</code> or <code>teardown</code> don't need to be present.</p>
<h2 id="why-using-a-sandbox"><a class="header" href="#why-using-a-sandbox">Why using a Sandbox?</a></h2>
<p>A <code>Sandbox</code> can help mitigating differences in benchmark results on different
machines. As long as <code>$TMP_DIR</code> is unset or set to <code>/tmp</code>, the temporary
directory has a constant length on unix machines (with the exception of android
which uses <code>/data/local/tmp</code>). The directory itself is created with a constant
length but random name like <code>/tmp/.a23sr8fk</code>.</p>
<p>It is not implausible that an executable has different event counts just because
the directory it is executed in has a different length. For example, if a member
of your project has set up the project in <code>/home/bob/workspace/our-project</code>
running the benchmarks in this directory, and the ci runs the benchmarks in
<code>/runner/our-project</code>, the event counts might differ. If possible, the
benchmarks should be run in a constant environment. For example <a href="benchmarks/binary_benchmarks/configuration/../important.html">clearing the
environment variables</a> is also such a measure.</p>
<p>Other good reasons for using a <code>Sandbox</code> are convenience, e.g. if you create
files during the <code>setup</code> and <code>Command</code> run and do not want to delete all files
manually. Or, maybe more importantly, if the <code>Command</code> is destructive and
deletes files, it is usually safer to run such a <code>Command</code> in a temporary
directory where it cannot cause damage to your or other file systems.</p>
<p>The <code>Sandbox</code> is deleted after the benchmark, regardless of whether the
benchmark run was successful or not. The latter is not guaranteed if you only
rely on <code>teardown</code>, as <code>teardown</code> is only executed if the <code>Command</code> returns
without error.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{
    binary_benchmark, binary_benchmark_group, main, BinaryBenchmarkConfig, Sandbox
};

fn create_file(path: &amp;str) {
    std::fs::write(path, "some content").unwrap();
}

#[binary_benchmark]
#[bench::foo(
    args = ("foo.txt"),
    config = BinaryBenchmarkConfig::default().sandbox(Sandbox::new(true)),
    setup = create_file
)]
fn bench_binary(path: &amp;str) -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
        .arg(path)
        .build()
}

binary_benchmark_group!(name = my_group; benchmarks = bench_binary);
<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>In this example, as part of the <code>setup</code>, the <code>create_file</code> function with the
argument <code>foo.txt</code> is executed in the <code>Sandbox</code> before the <code>Command</code> is
executed. The <code>Command</code> is executed in the same <code>Sandbox</code> and therefore the file
<code>foo.txt</code> with the content <code>some content</code> exists thanks to the <code>setup</code>. After
the execution of the <code>Command</code>, the <code>Sandbox</code> is completely removed, deleting
all files created during <code>setup</code>, the <code>Command</code> execution (and <code>teardown</code> if it
would have been present in this example).</p>
<p>Since <code>setup</code> is run in the sandbox, you can't copy fixtures from your project's
workspace into the sandbox that easily anymore. The <code>Sandbox</code> can be configured
to copy <code>fixtures</code> into the temporary directory with <code>Sandbox::fixtures</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{
    binary_benchmark, binary_benchmark_group, main, BinaryBenchmarkConfig, Sandbox
};

#[binary_benchmark]
#[bench::foo(
    args = ("foo.txt"),
    config = BinaryBenchmarkConfig::default()
        .sandbox(Sandbox::new(true)
            .fixtures(["benches/foo.txt"])),
)]
fn bench_binary(path: &amp;str) -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
        .arg(path)
        .build()
}

binary_benchmark_group!(name = my_group; benchmarks = bench_binary);
<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>The above will copy the fixture file <code>foo.txt</code> in the <code>benches</code> directory into
the sandbox root as <code>foo.txt</code>. Relative paths in <code>Sandbox::fixtures</code> are
interpreted relative to the workspace root. In a multi-crate workspace this is
the directory with the top-level <code>Cargo.toml</code> file. Paths in <code>Sandbox::fixtures</code>
are not limited to files, they can be directories, too.</p>
<p>If you have more complex demands, you can access the workspace root via the
environment variable <code>_WORKSPACE_ROOT</code> in <code>setup</code> and <code>teardown</code>. Suppose, there
is a fixture located in <code>/home/the_project/foo_crate/benches/fixtures/foo.txt</code>
with <code>the_project</code> being the workspace root and <code>foo_crate</code> a workspace member
with the <code>my-foo</code> executable. If the command is expected to create a file
<code>bar.json</code>, which needs further inspection after the benchmarks have run, let's
copy it into a temporary directory <code>tmp</code> (which may or may not exist) in
<code>foo_crate</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{
    binary_benchmark, binary_benchmark_group, main, BinaryBenchmarkConfig, Sandbox
};
use std::path::PathBuf;

fn copy_fixture(path: &amp;str) {
    let workspace_root = PathBuf::from(std::env::var_os("_WORKSPACE_ROOT").unwrap());
    std::fs::copy(
        workspace_root.join("foo_crate").join("benches").join("fixtures").join(path),
        path
    );
}

// This function will fail if `bar.json` does not exist, which is fine as this
// file is expected to be created by `my-foo`. So, if this file does not exist,
// an error will occur and the benchmark will fail. Although benchmarks are not
// expected to test the correctness of the application, the `teardown` can be
// used to check postconditions for a successful command run.
fn copy_back(path: &amp;str) {
    let workspace_root = PathBuf::from(std::env::var_os("_WORKSPACE_ROOT").unwrap());
    let dest_dir = workspace_root.join("foo_crate").join("tmp");
    if !dest_dir.exists() {
        std::fs::create_dir(&amp;dest_dir).unwrap();
    }
    std::fs::copy(path, dest_dir.join(path));
}

#[binary_benchmark]
#[bench::foo(
    args = ("foo.txt"),
    config = BinaryBenchmarkConfig::default().sandbox(Sandbox::new(true)),
    setup = copy_fixture,
    teardown = copy_back("bar.json")
)]
fn bench_binary(path: &amp;str) -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
        .arg(path)
        .build()
}

binary_benchmark_group!(name = my_group; benchmarks = bench_binary);
<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configure-the-exit-code-of-the-command"><a class="header" href="#configure-the-exit-code-of-the-command">Configure the exit code of the Command</a></h1>
<p>Usually, if a <code>Command</code> exits with a non-zero exit code, the whole benchmark run
fails and stops. If the exit code of the benchmarked <code>Command</code> is to be expected
different from <code>0</code>, the expected exit code can be set in
<code>BinaryBenchmarkConfig::exit_with</code> or <code>Command::exit_with</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{
     binary_benchmark, binary_benchmark_group, main, BinaryBenchmarkConfig, ExitWith
};

#[binary_benchmark]
// Here, we set the expected exit code of `my-foo` to 2
#[bench::exit_with_2(
    config = BinaryBenchmarkConfig::default().exit_with(ExitWith::Code(2))
)]
// Here, we don't know the exact exit code but know it is different from 0 (=success)
#[bench::exit_with_failure(
    config = BinaryBenchmarkConfig::default().exit_with(ExitWith::Failure)
)]
fn bench_binary() -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
}

binary_benchmark_group!(name = my_group; benchmarks = bench_binary);
<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="low-level-api"><a class="header" href="#low-level-api">Low-level api</a></h1>
<p>I'm not going into full detail of the low-level api here since it is fully
documented in the <a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/index.html">api
Documentation</a>.</p>
<h2 id="the-basic-structure"><a class="header" href="#the-basic-structure">The basic structure</a></h2>
<p>The entry point of the low-level api is the <code>binary_benchmark_group</code></p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{
     binary_benchmark, binary_benchmark_attribute, binary_benchmark_group, main,
     BinaryBenchmark, Bench
};

binary_benchmark_group!(
    name = my_group;
    benchmarks = |group: &amp;mut BinaryBenchmarkGroup| {
        group.binary_benchmark(BinaryBenchmark::new("bench_binary")
            .bench(Bench::new("some_id")
                .command(iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-foo"))
                    .arg("foo.txt")
                    .build()
                )
            )
        )
    }
);

<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>The low-level api mirrors the high-level api, "structifying" the macros.</p>
<p>The <code>binary_benchmark_group!</code> is also a struct now, the <code>BinaryBenchmarkGroup</code>.
It cannot be instantiated. Instead, it is passed as argument to the expression
of the <code>benchmarks</code> parameter in a <code>binary_benchmark_group</code>. You can choose any
name instead of <code>group</code>, we just have used <code>group</code> throughout the examples.</p>
<p>There's the shorter <code>benchmarks = |group| /* ... */</code> instead of <code>benchmarks = |group: &amp;mut BinaryBenchmarkGroup| /* ... */</code>. We use the more verbose variant
in the examples because it is more informative for benchmarking starters.</p>
<p>Furthermore, the <code>#[library_benchmark]</code> macro correlates with
<code>iai_callgrind::LibraryBenchmark</code> and <code>#[bench]</code> with <code>iai_callgrind::Bench</code>.
The parameters of the macros are now functions in the respective structs. The
return value of the benchmark function, the <code>iai-callgrind::Command</code>, is now
also a function <code>iai-callgrind::Bench::command</code>.</p>
<p>Note there is no <code>iai-callgrind::Benches</code> struct since specifying multiple
commands with <code>iai_callgrind::Bench::command</code> behaves exactly the same way as
the <code>#[benches]</code> attribute. So, the <code>file</code> parameter of <code>#[benches]</code> is a part
of <code>iai-callgrind::Bench</code> and can be used with the <code>iai-callgrind::Bench::file</code>
function.</p>
<h2 id="intermixing-high-level-and-low-level-api"><a class="header" href="#intermixing-high-level-and-low-level-api">Intermixing high-level and low-level api</a></h2>
<p>It is recommended to start with the high-level api using the
<code>#[binary_benchmark]</code> attribute, since you can fall back to the low-level api in
a few steps with the <code>binary_benchmark_attribute!</code> macro as shown below. The
other way around is much more involved.</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">macro_rules! env { ($m:tt) =&gt; {{ "/some/path" }} }
</span>use iai_callgrind::{
     binary_benchmark, binary_benchmark_attribute, binary_benchmark_group, main,
     BinaryBenchmark, Bench
};

#[binary_benchmark]
#[bench::some_id("foo")]
fn attribute_benchmark(arg: &amp;str) -&gt; iai_callgrind::Command {
    iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-binary"))
        .arg(arg)
        .build()
}

binary_benchmark_group!(
    name = low_level;
    benchmarks = |group: &amp;mut BinaryBenchmarkGroup| {
        group
            .binary_benchmark(binary_benchmark_attribute!(attribute_benchmark))
            .binary_benchmark(
                BinaryBenchmark::new("low_level_benchmark")
                    .bench(
                        Bench::new("some_id").command(
                            iai_callgrind::Command::new(env!("CARGO_BIN_EXE_my-binary"))
                                .arg("bar")
                                .build()
                        )
                    )
            )
    }
);

<span class="boring">fn main() {
</span>main!(binary_benchmark_groups = low_level);
<span class="boring">}</span></code></pre></pre>
<p>As shown above, there's no need to transcribe the function <code>attribute_benchmark</code>
with the <code>#[binary_benchmark]</code> attribute into the low-level api structures. Just
keep it as it is and add it to a the <code>group</code> with
<code>group.binary_benchmark(binary_benchmark_attribute(attribute_benchmark))</code>.
That's it! You can continue hacking on your benchmarks in the low-level api.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="more-examples-needed"><a class="header" href="#more-examples-needed">More examples needed?</a></h1>
<p>As in <a href="benchmarks/binary_benchmarks/../library_benchmarks/examples.html">library benchmarks</a>, I'm referring
here to the <a href="https://github.com/iai-callgrind/iai-callgrind">github repository</a>.
The binary benchmarks functionality of Iai-Callgrind is tested with system tests
in the private
<a href="https://github.com/iai-callgrind/iai-callgrind/tree/main/benchmark-tests/benches/test_lib_bench">benchmark-tests</a>
package.</p>
<p>Each system test there can serve you as an example, but for a fully documented
and commented one see
<a href="https://github.com/iai-callgrind/iai-callgrind/blob/main/benchmark-tests/benches/test_bin_bench/intro/test_bin_bench_intro.rs">here</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable MD041 MD033 -->
<h1 id="performance-regressions"><a class="header" href="#performance-regressions">Performance Regressions</a></h1>
<p>With Iai-Callgrind you can define limits for each event kinds over which a
performance regression can be assumed. Per default, Iai-Callgrind does not
perform default regression checks and you have to opt-in with a
<code>RegressionConfig</code> at benchmark level with a <code>LibraryBenchmarkConfig</code> or
<code>BinaryBenchmarkConfig</code> or at a global level with <a href="./cli_and_env/basics.html">Command-line arguments or
Environment variables</a>.</p>
<h2 id="define-a-performance-regression"><a class="header" href="#define-a-performance-regression">Define a performance regression</a></h2>
<p>A performance regression check consists of an <code>EventKind</code> and a percentage. If
the percentage is negative, then a regression is assumed to be below this limit.</p>
<p>The default <code>EventKind</code> is <code>EventKind::Ir</code> with a value of <code>+10%</code>.</p>
<p>For example, in a <a href="./benchmarks/library_benchmarks/configuration.html">Library
Benchmark</a>, define a limit of
<code>+5%</code> for the total instructions executed (the <code>Ir</code> event kind) in all
benchmarks of this file :</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(_: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec![] } }
</span>use iai_callgrind::{
    library_benchmark, library_benchmark_group, main, LibraryBenchmarkConfig,
    RegressionConfig, EventKind
};
use std::hint::black_box;

#[library_benchmark]
fn bench_library() -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(vec![3, 2, 1]))
}

library_benchmark_group!(name = my_group; benchmarks = bench_library);

<span class="boring">fn main() {
</span>main!(
    config = LibraryBenchmarkConfig::default()
        .regression(
            RegressionConfig::default()
                .limits([(EventKind::Ir, 5.0)])
        );
    library_benchmark_groups = my_group
);
<span class="boring">}</span></code></pre></pre>
<p>Now, if the comparison of the <code>Ir</code> events of the current <code>bench_library</code>
benchmark run with the previous run results in an increase of over 5%, the
benchmark fails. Please, also have a look at the <a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/struct.RegressionConfig.html"><code>api docs</code></a>
for further configuration options.</p>
<p>Running the benchmark from above the first time results in the following output:</p>
<pre><code class="hljs"><span style="color:#0A0">my_benchmark::my_group::bench_library</span>
  Instructions:     <b>            215</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>            288</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              0</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              7</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>            295</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>            533</b>|N/A             (<span style="color:#555">*********</span>)</code></pre>
<p>Let's assume there's a change in <code>my_lib::bubble_sort</code> which has increased the
instruction counts, then running the benchmark again results in an output
something similar to this:</p>
<pre><code class="hljs"><span style="color:#0A0">my_benchmark::my_group::bench_library</span>
  Instructions:     <b>            281</b>|215             (<b><span style="color:#F55">+30.6977%</span></b>) [<b><span style="color:#F55">+1.30698x</span></b>]
  L1 Hits:          <b>            374</b>|288             (<b><span style="color:#F55">+29.8611%</span></b>) [<b><span style="color:#F55">+1.29861x</span></b>]
  L2 Hits:          <b>              0</b>|0               (<span style="color:#555">No change</span>)
  RAM Hits:         <b>              8</b>|7               (<b><span style="color:#F55">+14.2857%</span></b>) [<b><span style="color:#F55">+1.14286x</span></b>]
  Total read+write: <b>            382</b>|295             (<b><span style="color:#F55">+29.4915%</span></b>) [<b><span style="color:#F55">+1.29492x</span></b>]
  Estimated Cycles: <b>            654</b>|533             (<b><span style="color:#F55">+22.7017%</span></b>) [<b><span style="color:#F55">+1.22702x</span></b>]
Performance has <b><span style="color:#F55">regressed</span></b>: <b>Instructions</b> (281 > 215) regressed by <b><span style="color:#F55">+30.6977%</span></b> (><span style="color:#555">+5.00000</span>)
iai_callgrind_runner: <b><span style="color:#A00">Error</span></b>: Performance has regressed.
error: bench failed, to rerun pass `-p the-crate --bench my_benchmark`

Caused by:
  process didn't exit successfully: `/path/to/your/project/target/release/deps/my_benchmark-a9b36fec444944bd --bench` (exit status: 1)
error: Recipe `bench-test` failed on line 175 with exit code 1</code></pre>
<h2 id="which-event-to-choose-to-measure-performance-regressions"><a class="header" href="#which-event-to-choose-to-measure-performance-regressions">Which event to choose to measure performance regressions?</a></h2>
<p>If in doubt, the definite answer is <code>Ir</code> (instructions executed). If <code>Ir</code> event
counts decrease noticeable the function (binary) runs faster. The inverse
statement is also true: If the <code>Ir</code> counts increase noticeable, there's a
slowdown of the function (binary).</p>
<p>These statements are not so easy to transfer to <code>Estimated Cycles</code> and the other
event counts. But, depending on the scenario and the function (binary) under
test, it can be reasonable to define more regression checks.</p>
<h2 id="who-actually-uses-instructions-to-measure-performance"><a class="header" href="#who-actually-uses-instructions-to-measure-performance">Who actually uses instructions to measure performance?</a></h2>
<p>The ones known to the author of this humble guide are</p>
<ul>
<li><a href="https://sqlite.org/cpu.html#performance_measurement">SQLite</a>: They use mainly
cpu instructions to measure performance improvements (and regressions).</li>
<li>Also in benchmarks of the <a href="https://github.com/rust-lang/rustc-perf">rustc</a>
compiler, instruction counts play a great role. But, they also use cache
metrics and cycles.</li>
</ul>
<p>If you know of others, please feel free to
<a href="https://github.com/iai-callgrind/iai-callgrind/master/docs/src/regressions.md">add</a>
them to this list.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="other-valgrind-tools"><a class="header" href="#other-valgrind-tools">Other Valgrind Tools</a></h1>
<p>In addition to the default benchmarks, you can use the Iai-Callgrind framework
to run other Valgrind profiling <code>Tool</code>s like <code>DHAT</code>, <code>Massif</code> and the
experimental <code>BBV</code> but also <code>Memcheck</code>, <code>Helgrind</code> and <code>DRD</code> if you need to
check memory and thread safety of benchmarked code. See also the <a href="https://valgrind.org/docs/manual/manual.html">Valgrind User
Manual</a> for more details and
command line arguments. The additional tools can be specified in a
<code>LibraryBenchmarkConfig</code> or <code>BinaryBenchmarkConfig</code>. For example to run <code>DHAT</code>
for all library benchmarks in addition to <code>Callgrind</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(_: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec![] } }
</span>use iai_callgrind::{
    library_benchmark, library_benchmark_group, main, LibraryBenchmarkConfig, 
    Tool, ValgrindTool
};
use std::hint::black_box;

#[library_benchmark]
fn bench_library() -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(vec![3, 2, 1]))
}

library_benchmark_group!(name = my_group; benchmarks = bench_library);

<span class="boring">fn main() {
</span>main!(
    config = LibraryBenchmarkConfig::default()
        .tool(Tool::new(ValgrindTool::DHAT));
    library_benchmark_groups = my_group
);
<span class="boring">}</span></code></pre></pre>
<p>All tools which produce an <code>ERROR SUMMARY</code> <code>(Memcheck, DRD, Helgrind)</code> have
<a href="https://valgrind.org/docs/manual/manual-core.html#manual-core.erropts"><code>--error-exitcode=201</code></a>
set, so if there are any errors, the benchmark run fails with <code>201</code>. You can
overwrite this default with</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::{Tool, ValgrindTool};

Tool::new(ValgrindTool::Memcheck).args(["--error-exitcode=0"]);
<span class="boring">}</span></code></pre></pre>
<p>which would restore the default of <code>0</code> from valgrind.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="valgrind-client-requests-1"><a class="header" href="#valgrind-client-requests-1">Valgrind Client Requests</a></h1>
<p>Iai-Callgrind ships with its own interface to the <a href="https://valgrind.org/docs/manual/manual-core-adv.html#manual-core-adv.clientreq">Valgrind's Client Request
Mechanism</a>.
Iai-Callgrind's client requests have zero overhead (relative to the "C"
implementation of Valgrind) on many targets which are also natively supported by
valgrind. In short, Iai-Callgrind provides a complete and performant
implementation of Valgrind Client Requests.</p>
<h2 id="installation"><a class="header" href="#installation">Installation</a></h2>
<p>Client requests are deactivated by default but can be activated with the
<code>client_requests</code> feature.</p>
<pre><code class="language-toml">[dev-dependencies]
iai-callgrind = { version = "0.13.3", features = ["client_requests"] }
</code></pre>
<p>If you need the client requests in your production code, you don't want them to
do anything when not running under valgrind with Iai-Callgrind benchmarks. You
can achieve that by adding Iai-Callgrind with the <code>client_requests_defs</code> feature
to your runtime dependencies and with the <code>client_requests</code> feature to your
<code>dev-dependencies</code> like so:</p>
<pre><code class="language-toml">[dependencies]
iai-callgrind = { version = "0.13.3", default-features = false, features = [
    "client_requests_defs"
] }

[dev-dependencies]
iai-callgrind = { version = "0.13.3", features = ["client_requests"] }
</code></pre>
<p>With just the <code>client_requests_defs</code> feature activated, the client requests
compile down to nothing and don't add any overhead to your production code. It
simply provides the "definitions", method signatures and macros without body.
Only with the activated <code>client_requests</code> feature they will be actually
executed. Note that the client requests do not depend on any other part of
Iai-Callgrind, so you could even use the client requests without the rest of
Iai-Callgrind.</p>
<p>When building Iai-Callgrind with client requests, the valgrind header files must
exist in your standard include path (most of the time <code>/usr/include</code>). This is
usually the case if you've installed valgrind with your distribution's package
manager. If not, you can point the <code>IAI_CALLGRIND_VALGRIND_INCLUDE</code> or
<code>IAI_CALLGRIND_&lt;triple&gt;_VALGRIND_INCLUDE</code> environment variables to the include
path. So, if the headers can be found in <code>/home/foo/repo/valgrind/{valgrind.h, callgrind.h, ...}</code>, the correct include path would be
<code>IAI_CALLGRIND_VALGRIND_INCLUDE=/home/foo/repo</code> (not <code>/home/foo/repo/valgrind</code>)</p>
<h2 id="usage"><a class="header" href="#usage">Usage</a></h2>
<p>Use them in your code for example like so:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::client_requests;

<span class="boring">fn main() {
</span>fn main() {
    // Start callgrind event counting if not already started earlier
    client_requests::callgrind::start_instrumentation();

    // do something important

    // Switch event counting off
    client_requests::callgrind::stop_instrumentation();
}
<span class="boring">}</span></code></pre></pre>
<h3 id="library-benchmarks-1"><a class="header" href="#library-benchmarks-1">Library Benchmarks</a></h3>
<p>In <a href="./benchmarks/library_benchmarks.html">library benchmarks</a> you might need to
use <a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/enum.EntryPoint.html"><code>EntryPoint::None</code></a> in order to make the client requests work
as expected:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::{main, library_benchmark_group, library_benchmark};
use std::hint::black_box;

pub mod my_lib {
     #[inline(never)]
     fn bubble_sort(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
         // The algorithm
<span class="boring">       input
</span>     }

     pub fn pre_bubble_sort(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
         println!("Doing something before the function call");
         iai_callgrind::client_requests::callgrind::start_instrumentation();

         let result = bubble_sort(input);

         iai_callgrind::client_requests::callgrind::stop_instrumentation();
         result
     }
}

#[library_benchmark]
#[bench::small(vec![3, 2, 1])]
#[bench::bigger(vec![5, 4, 3, 2, 1])]
fn bench_function(array: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::pre_bubble_sort(array))
}

library_benchmark_group!(name = my_group; benchmarks = bench_function);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>The default <a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/enum.EntryPoint.html"><code>EntryPoint</code></a> sets the <a href="https://valgrind.org/docs/manual/cl-manual.html#cl-manual.options"><code>--toggle-collect</code></a> to the benchmark function (here <code>bench_function</code>) and
<code>--collect-at-start=no</code>. So, <code>Callgrind</code> starts collecting the events when
entering the benchmark function, not the moment <code>start_instrumentation</code> is
called. This behaviour can be remedied with <code>EntryPoint::None</code>:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::{
    main, library_benchmark_group, library_benchmark, LibraryBenchmarkConfig,
    client_requests, EntryPoint
};
use std::hint::black_box;

pub mod my_lib {
     #[inline(never)]
     fn bubble_sort(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
         // The algorithm
<span class="boring">       input
</span>     }

     pub fn pre_bubble_sort(input: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
         println!("Doing something before the function call");
         iai_callgrind::client_requests::callgrind::start_instrumentation();

         let result = bubble_sort(input);

         iai_callgrind::client_requests::callgrind::stop_instrumentation();
         result
     }
}

#[library_benchmark(
    config = LibraryBenchmarkConfig::default()
        .raw_callgrind_args(["--collect-at-start=no"])
        .entry_point(EntryPoint::None)
)]
#[bench::small(vec![3, 2, 1])]
#[bench::bigger(vec![5, 4, 3, 2, 1])]
fn bench_function(array: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::pre_bubble_sort(array))
}

library_benchmark_group!(name = my_group; benchmarks = bench_function);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>As the standard toggle is now switched off and the option
<code>--collect-at-start=no</code> is also omitted, you must specify
<code>--collect-at-start=no</code> manually in
<code>LibraryBenchmarkConfig::raw_callgrind_args</code>.</p>
<p>Please see the
<a href="https://docs.rs/iai-callgrind/0.13.3/iai_callgrind/client_requests"><code>docs</code></a> for
more details!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="callgrind-flamegraphs"><a class="header" href="#callgrind-flamegraphs">Callgrind Flamegraphs</a></h1>
<p>Flamegraphs are opt-in and can be created if you pass a <code>FlamegraphConfig</code> to
the <code>BinaryBenchmarkConfig</code> or <code>LibraryBenchmarkConfig</code>. Callgrind flamegraphs
are meant as a complement to valgrind's visualization tools
<code>callgrind_annotate</code> and <code>kcachegrind</code>.</p>
<p>For example create all kind of flamegraphs for all benchmarks in a library
benchmark:</p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(_: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec![] } }
</span>use iai_callgrind::{
    library_benchmark, library_benchmark_group, main, LibraryBenchmarkConfig,
    FlamegraphConfig
};
use std::hint::black_box;

#[library_benchmark]
fn bench_library() -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(vec![3, 2, 1]))
}

library_benchmark_group!(name = my_group; benchmarks = bench_library);

<span class="boring">fn main() {
</span>main!(
    config = LibraryBenchmarkConfig::default()
        .flamegraph(FlamegraphConfig::default());
    library_benchmark_groups = my_group
);
<span class="boring">}</span></code></pre></pre>
<p>The produced flamegraph <code>*.svg</code> files are located next to the respective
callgrind output file in the <code>target/iai</code>
<a href="./cli_and_env/output/out_directory.html">directory</a>.</p>
<h2 id="regular-flamegraphs"><a class="header" href="#regular-flamegraphs">Regular Flamegraphs</a></h2>
<p>Regular callgrind flamegraphs show the inclusive costs for functions and a
single <code>EventKind</code> (default is <code>EventKind::Ir</code>), similar to
<code>callgrind_annotate</code>. Suppose the example from above is stored in a benchmark
<code>iai_callgrind_benchmark</code>:</p>
<p><img src="./images/flamegraph_regular.svg" alt="Regular Flamegraph" /></p>
<p>If you open this image in a new tab, you can play around with the svg.</p>
<h2 id="differential-flamegraphs"><a class="header" href="#differential-flamegraphs">Differential Flamegraphs</a></h2>
<p>Differential flamegraphs facilitate a deeper understanding of code sections
which cause a bottleneck or a performance regressions etc.</p>
<p><img src="./images/flamegraph_diff.svg" alt="Differential Flamegraph" /></p>
<p>We simulated a small change in <code>bubble_sort</code> and in the differential flamegraph
you can spot fairly easily where the increase of <code>Instructions</code> is happening.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="basic-usage"><a class="header" href="#basic-usage">Basic usage</a></h1>
<p>It's possible to pass arguments to Iai-Callgrind separated by <code>--</code> (<code>cargo bench -- ARGS</code>). If you're running into the error <code>Unrecognized Option</code>, see
<a href="cli_and_env/../troubleshooting/running-cargo-bench-results-in-an-unrecognized-option-error.html">Troubleshooting</a>.
For a complete rundown of possible arguments, execute <code>cargo bench --bench &lt;benchmark&gt; -- --help</code>. Almost all command-line arguments have a corresponding
environment variable. The environment variables which don't have a corresponding
command-line argument are:</p>
<ul>
<li><code>IAI_CALLGRIND_COLOR</code>: <a href="cli_and_env/./output/color.html">Control the colored output of Iai-Callgrind</a> (Default
is <code>auto</code>)</li>
<li><code>IAI_CALLGRIND_LOG</code>: <a href="cli_and_env/./output/logging.html">Define the log level</a> (Default is <code>WARN</code>)</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable MD041 MD033 -->
<h1 id="comparing-with-baselines"><a class="header" href="#comparing-with-baselines">Comparing with baselines</a></h1>
<p>Usually, two consecutive benchmark runs let Iai-Callgrind compare these two
runs. It's sometimes desirable to compare the current benchmark run against a
static reference, instead. For example, if you're working longer on the
implementation of a feature, you may wish to compare against a baseline from
another branch or the commit from which you started off hacking on your new
feature to make sure you haven't introduced performance regressions.
Iai-Callgrind offers such custom baselines. If you are familiar with
<a href="https://bheisler.github.io/criterion.rs/book/user_guide/command_line_options.html#baselines">criterion.rs</a>,
the following command line arguments should also be very familiar to you:</p>
<ul>
<li><code>--save-baseline=NAME</code> (env: <code>IAI_CALLGRIND_SAVE_BASELINE</code>): Compare against
the <code>NAME</code> baseline if present and then overwrite it.</li>
<li><code>--baseline=NAME</code> (env: <code>IAI_CALLGRIND_BASELINE</code>): Compare against the <code>NAME</code>
baseline without overwriting it</li>
<li><code>--load-baseline=NAME</code> (env: <code>IAI_CALLGRIND_LOAD_BASELINE</code>): Load the <code>NAME</code>
baseline as the <code>new</code> data set instead of creating a new one. This option
needs also <code>--baseline=NAME</code> to be present.</li>
</ul>
<p>If <code>NAME</code> is not present, <code>NAME</code> defaults to <code>default</code>.</p>
<p>For example to create a static reference from the main branch and compare it:</p>
<pre><code class="language-shell">git checkout main
cargo bench --bench &lt;benchmark&gt; -- --save-baseline=main
git checkout feature
# ... HACK ... HACK
cargo bench --bench &lt;benchmark&gt; -- --baseline main
</code></pre>
<p>Sticking to the above execution sequence,</p>
<pre><code class="language-shell">cargo bench --bench my_benchmark -- --save-baseline=main
</code></pre>
<p>prints something like that with an additional line <code>Baselines</code> in the output.</p>
<pre><code class="hljs"><span style="color:#0A0">my_benchmark::my_group::bench_library</span>
  Baselines:        <b>           main</b>|main
  Instructions:     <b>            280</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>            374</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              1</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>              6</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>            381</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>            589</b>|N/A             (<span style="color:#555">*********</span>)</code></pre>
<p>After you've made some changes to your code, running</p>
<pre><code class="language-shell">cargo bench --bench my_benchmark -- --baseline=main`
</code></pre>
<p>prints something like the following:</p>
<pre><code class="hljs"><span style="color:#0A0">my_benchmark::my_group::bench_library</span>
  Baselines:                       |main
  Instructions:     <b>            214</b>|280             (<b><span style="color:#42c142">-23.5714%</span></b>) [<b><span style="color:#42c142">-1.30841x</span></b>]
  L1 Hits:          <b>            287</b>|374             (<b><span style="color:#42c142">-23.2620%</span></b>) [<b><span style="color:#42c142">-1.30314x</span></b>]
  L2 Hits:          <b>              1</b>|1               (<span style="color:#555">No change</span>)
  RAM Hits:         <b>              6</b>|6               (<span style="color:#555">No change</span>)
  Total read+write: <b>            294</b>|381             (<b><span style="color:#42c142">-22.8346%</span></b>) [<b><span style="color:#42c142">-1.29592x</span></b>]
  Estimated Cycles: <b>            502</b>|589             (<b><span style="color:#42c142">-14.7708%</span></b>) [<b><span style="color:#42c142">-1.17331x</span></b>]</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="controlling-the-output-of-iai-callgrind"><a class="header" href="#controlling-the-output-of-iai-callgrind">Controlling the output of Iai-Callgrind</a></h1>
<p>This section describes command-line options and environment variables which
influence the terminal, file and logging output of Iai-Callgrind.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="customize-the-output-directory"><a class="header" href="#customize-the-output-directory">Customize the output directory</a></h1>
<p>All output files of Iai-Callgrind are usually stored using the following scheme:</p>
<p><code>$WORKSPACE_ROOT/target/iai/$PACKAGE_NAME/$BENCHMARK_FILE/$GROUP/$BENCH_FUNCTION.$BENCH_ID</code></p>
<p>This directory structure can partly be changed with the following options.</p>
<h2 id="callgrind-home"><a class="header" href="#callgrind-home">Callgrind Home</a></h2>
<p>Per default, all benchmark output files are stored under the
<code>$WORKSPACE_ROOT/target/iai</code> directory tree. This home directory can be changed
with the <code>IAI_CALLGRIND_HOME</code> environment variable or the command-line argument
<code>--home</code>. The command-line argument overwrites the value of the environment
variable. For example to store all files under the <code>/tmp/iai-callgrind</code>
directory you can use <code>IAI_CALLGRIND_HOME=/tmp/iai-callgrind</code> or <code>cargo bench -- --home=/tmp/iai-callgrind</code>.</p>
<h2 id="separate-targets"><a class="header" href="#separate-targets">Separate targets</a></h2>
<p>If you're running the benchmarks on different targets, it's necessary to
separate the output files of the benchmark runs per target or else you could end
up comparing the benchmarks with the wrong target leading to strange results.
You can achieve this with different baselines per target, but it's much less
painful to separate the output files by target with the <code>--separate-targets</code>
command-line argument or setting the environment variable
<code>IAI_CALLGRIND_SEPARATE_TARGETS=yes</code>). The output directory structure
changes from</p>
<p><code>target/iai/$PACKAGE_NAME/$BENCHMARK_FILE/$GROUP/$BENCH_FUNCTION.$BENCH_ID</code></p>
<p>to</p>
<p><code>target/iai/$TARGET_TRIPLE/$PACKAGE_NAME/$BENCHMARK_FILE/$GROUP/$BENCH_FUNCTION.$BENCH_ID</code></p>
<p>For example, assuming the library benchmark file name is <code>bench_file</code> in the
package <code>my_package</code></p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span><span class="boring">mod my_lib { pub fn bubble_sort(_: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; { vec![] } }
</span>use iai_callgrind::{main, library_benchmark_group, library_benchmark};
use std::hint::black_box;

#[library_benchmark]
#[bench::short(vec![4, 3, 2, 1])]
fn bench_bubble_sort(values: Vec&lt;i32&gt;) -&gt; Vec&lt;i32&gt; {
    black_box(my_lib::bubble_sort(values))
}

library_benchmark_group!(name = my_group; benchmarks = bench_bubble_sort);

<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>Without <code>--separate-targets</code>:</p>
<p><code>target/iai/my_package/bench_file/my_group/bench_bubble_sort.short</code></p>
<p>and with <code>--separate-targets</code> assuming you're running the benchmark on the
<code>x86_64-unknown-linux-gnu</code> target:</p>
<p><code>target/iai/x86_64-unknown-linux-gnu/my_package/bench_file/my_group/bench_bubble_sort.short</code></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="machine-readable-output"><a class="header" href="#machine-readable-output">Machine-readable output</a></h1>
<p>With <code>--output-format=default|json|pretty-json</code> (env:
<code>IAI_CALLGRIND_OUTPUT_FORMAT</code>) you can change the terminal output format to the
machine-readable json format. The json schema fully describing the json output
is stored in
<a href="https://github.com/iai-callgrind/iai-callgrind/blob/main/iai-callgrind-runner/schemas/summary.v2.schema.json">summary.v2.schema.json</a>.
Each line of json output (if not <code>pretty-json</code>) is a summary of a single
benchmark and you may want to combine all benchmarks in an array. You can do so
for example with <code>jq</code></p>
<p><code>cargo bench -- --output-format=json | jq -s</code></p>
<p>which transforms <code>{...}\n{...}</code> into <code>[{...},{...}]</code>.</p>
<p>Instead of, or in addition to changing the terminal output, it's possible to
save a summary file for each benchmark with <code>--save-summary=json|pretty-json</code>
(env: <code>IAI_CALLGRIND_SAVE_SUMMARY</code>). The <code>summary.json</code> files are stored next to
the usual benchmark output files in the <code>target/iai</code> directory.</p>
<div style="break-before: page; page-break-before: always;"></div><!-- markdownlint-disable MD041 MD033 -->
<h1 id="showing-terminal-output-of-benchmarks"><a class="header" href="#showing-terminal-output-of-benchmarks">Showing terminal output of benchmarks</a></h1>
<p>Per default, all terminal output of the benchmark function, <code>setup</code> and
<code>teardown</code> is captured and therefore not shown during a benchmark run.</p>
<h2 id="using-the-log-level"><a class="header" href="#using-the-log-level">Using the log level</a></h2>
<p>The most basic possibility to show any captured output, is to use
<a href="cli_and_env/output/./logging.html"><code>IAI_CALLGRIND_LOG=info</code></a>. This includes a lot of other output,
too.</p>
<h2 id="tell-iai-callgrind-to-not-capture-the-output"><a class="header" href="#tell-iai-callgrind-to-not-capture-the-output">Tell Iai-Callgrind to not capture the output</a></h2>
<p>Another nicer possibility is, to tell Iai-Callgrind to not capture output with
the <code>--nocapture</code> (env: <code>IAI_CALLGRIND_NOCAPTURE</code>) option. This is currently
restricted to the <code>callgrind</code> run to prevent showing the same output multiple
times. So, any terminal output of <a href="cli_and_env/output/../../tools.html">other tool runs</a> is still
captured.</p>
<p>The <code>--nocapture</code> flag takes the special values <code>stdout</code> and <code>stderr</code> in
addition to <code>true</code> and <code>false</code>:</p>
<p><code>--nocapture=true|false|stdout|stderr</code></p>
<p>In the <code>--nocapture=stdout</code> case, terminal output to <code>stdout</code> is not captured
and shown during the benchmark run but output to <code>stderr</code> is discarded.
Likewise, <code>--nocapture=stderr</code> shows terminal output to <code>stderr</code> but discards
output to <code>stdout</code>.</p>
<p>Let's take as example a library benchmark <code>benches/my_benchmark.rs</code></p>
<pre><pre class="playground"><code class="language-rust edition2021"><span class="boring">extern crate iai_callgrind;
</span>use iai_callgrind::{library_benchmark, library_benchmark_group, main};
use std::hint::black_box;

fn print_to_stderr(value: u64) {
    eprintln!("Error output during teardown: {value}");
}

fn add_10_and_print(value: u64) -&gt; u64 {
    let value = value + 10;
    println!("Output to stdout: {value}");

    value
}

#[library_benchmark]
#[bench::some_id(args = (10), teardown = print_to_stderr)]
fn bench_library(value: u64) -&gt; u64 {
    black_box(add_10_and_print(value))
}

library_benchmark_group!(name = my_group; benchmarks = bench_library);
<span class="boring">fn main() {
</span>main!(library_benchmark_groups = my_group);
<span class="boring">}</span></code></pre></pre>
<p>If the above benchmark is run with <code>cargo bench --bench my_benchmark -- --nocapture</code>, the output of Iai-Callgrind will look like this:</p>
<pre><code class="hljs"><span style="color:#0A0">my_benchmark::my_group::bench_library</span> <span style="color:#0AA">some_id</span><span style="color:#0AA">:</span><b><span style="color:#00A">10</span></b>
Output to stdout: 20
Error output during teardown: 20
<span style="color:#A50">-</span> <span style="color:#A50">end of stdout/stderr</span>
  Instructions:     <b>            851</b>|N/A             (<span style="color:#555">*********</span>)
  L1 Hits:          <b>           1193</b>|N/A             (<span style="color:#555">*********</span>)
  L2 Hits:          <b>              5</b>|N/A             (<span style="color:#555">*********</span>)
  RAM Hits:         <b>             66</b>|N/A             (<span style="color:#555">*********</span>)
  Total read+write: <b>           1264</b>|N/A             (<span style="color:#555">*********</span>)
  Estimated Cycles: <b>           3528</b>|N/A             (<span style="color:#555">*********</span>)</code></pre>
<p>Everything between the headline and the <code>- end of stdout/stderr</code> line is output
from your benchmark. The <code>- end of stdout/stderr</code> line changes depending on the
options you have given. For example in the <code>--nocapture=stdout</code> case this line
indicates your chosen option with <code>- end of stdout</code>.</p>
<p>Note that independently of the value of the <code>--nocapture</code> option, all logging
output of a valgrind tool itself is stored in files in the output directory of
the benchmark. Since Iai-Callgrind needs the logging output of valgrind tools
stored in files, there is no option to disable the creation of these log files.
But, if anything goes sideways you might be glad to have the log files around.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="changing-the-color-output"><a class="header" href="#changing-the-color-output">Changing the color output</a></h1>
<p>The terminal output is colored per default but follows the value for the
<code>IAI_CALLGRIND_COLOR</code> environment variable. If <code>IAI_CALLGRIND_COLOR</code> is not set,
<code>CARGO_TERM_COLOR</code> is also tried. Accepted values are:</p>
<p><code>always</code>, <code>never</code>, <code>auto</code> (default).</p>
<p>So, disabling colors can be achieved with setting <code>IAI_CALLGRIND_COLOR</code> or
<code>CARGO_TERM_COLOR=never</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="changing-the-logging-output"><a class="header" href="#changing-the-logging-output">Changing the logging output</a></h1>
<p>Iai-Callgrind uses <a href="https://crates.io/crates/env_logger">env_logger</a> and the
default logging level <code>WARN</code>. To set the logging level to something different,
set the environment variable <code>IAI_CALLGRIND_LOG</code> for example to
<code>IAI_CALLGRIND_LOG=DEBUG</code>. Accepted values are:</p>
<p><code>error</code>, <code>warn</code> (default), <code>info</code>, <code>debug</code>, <code>trace</code>.</p>
<p>The logging output is colored per default but follows the <a href="cli_and_env/output/./color.html">Color
settings</a>.</p>
<p>See also the <a href="https://docs.rs/env_logger/latest">documentation</a> of <code>env_logger</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="im-getting-the-error-sentinel--not-found"><a class="header" href="#im-getting-the-error-sentinel--not-found">I'm getting the error Sentinel ... not found</a></h1>
<p>You've most likely disabled creating debug symbols in your cargo <code>bench</code>
profile. This can originate in an option you've added to the <code>release</code> profile
since the <code>bench</code> profile inherits the <code>release</code> profile. For example, if you've
added <code>strip = true</code> to your <code>release</code> profile which is perfectly fine, you need
to disable this option in your <code>bench</code> profile to be able to run Iai-Callgrind
benchmarks.</p>
<p>See also the <a href="troubleshooting/../installation/prerequisites.html#debug-symbols">Debug Symbols</a>
section in Installation/Prerequisites.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="running-cargo-bench-results-in-an-unrecognized-option-error"><a class="header" href="#running-cargo-bench-results-in-an-unrecognized-option-error">Running cargo bench results in an "Unrecognized Option" error</a></h1>
<p>For</p>
<pre><code class="language-shell">cargo bench -- --some-valid-arg
</code></pre>
<p>to work you can either specify the
benchmark with <code>--bench BENCHMARK</code>, for example</p>
<pre><code class="language-shell">cargo bench --bench my_iai_benchmark -- --callgrind-args="--collect-bus=yes"
</code></pre>
<p>or add the following to your <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">[lib]
bench = false
</code></pre>
<p>and if you have binaries</p>
<pre><code class="language-toml">[[bin]]
name = "my-binary"
path = "src/bin/my-binary.rs"
bench = false
</code></pre>
<p>Setting <code>bench = false</code> disables the creation of the implicit default <code>libtest</code>
harness which is added even if you haven't used <code>#[bench]</code> functions in your
library or binary. Naturally, the default harness doesn't know of the
Iai-Callgrind arguments and aborts execution printing the <code>Unrecognized Option</code> error.</p>
<p>If you cannot or don't want to add <code>bench = false</code> to your <code>Cargo.toml</code>, you can
alternatively use environment variables. For every <a href="troubleshooting/../cli_and_env/basics.html">command-line
argument</a> exists a corresponding environment variable.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="comparison-of-iai-callgrind-with-criterion-rs"><a class="header" href="#comparison-of-iai-callgrind-with-criterion-rs">Comparison of Iai-Callgrind with Criterion-rs</a></h1>
<p>This is a comparison with
<a href="https://github.com/bheisler/criterion.rs?tab=readme-ov-file">Criterion-rs</a> but
some of the points in Pros and Cons also apply to other wall-clock time based
benchmarking frameworks.</p>
<p>Iai-Callgrind Pros:</p>
<ul>
<li>
<p>Iai-Callgrind can give answers that are repeatable to 7 or more significant
digits. In comparison, actual (wall-clock) run times are scarcely repeatable
beyond one significant digit.</p>
<p>This allows to implement and measure "microoptimizations". Typical
microoptimizations reduce the number of CPU cycles by <code>0.1%</code> or <code>0.05%</code> or
even less. Such improvements are impossible to measure with real-world
timings. But hundreds or thousands of microoptimizations add up, resulting in
measurable real-world performance gains.<sup class="footnote-reference"><a href="#note">1</a></sup></p>
</li>
<li>
<p>Iai-Callgrind can work reliably in noisy environments especially in CI
environments from providers like GitHub Actions or Travis-CI, where
Criterion-rs cannot.</p>
</li>
<li>
<p>The benchmark api of Iai-Callgrind is simple, intuitive and allows for a much
more concise and clearer structure of benchmarks.</p>
</li>
<li>
<p>Iai-Callgrind can benchmark functions in binary crates.</p>
</li>
<li>
<p>Iai-Callgrind can benchmark private functions.</p>
</li>
<li>
<p>Although Callgrind adds runtime overhead, running each benchmark exactly once
is still usually much faster than Criterion-rs' statistical measurements.</p>
</li>
<li>
<p>Criterion-rs creates plots and graphs about the averages, median etc. which
adds considerable execution time to the execution time for each benchmark.
Iai-Callgrind doesn't need any of these plots, since it can collect all its
metrics in a single run.</p>
</li>
<li>
<p>Iai-Callgrind generates profile output from the benchmark without further
effort.</p>
</li>
<li>
<p>With Iai-Callgrind you have native access to all the possibilities of all
Valgrind tools, including Valgrind Client Requests.</p>
</li>
</ul>
<p>Iai-Callgrind/Criterion-rs Mixed:</p>
<ul>
<li>Although it is usually not significant, due to the high precision of the
Iai-Callgrind measurements changes in the benchmarks themselves like adding an
additional benchmark case can have an effect on the other benchmarks.
Iai-Callgrind can only try to reduce these effects to a minimum but never
completely eliminate them. Criterion-rs does not have this problem because it
cannot detect such small changes.</li>
</ul>
<p>Iai-Callgrind Cons:</p>
<ul>
<li>Iai-Callgrind's measurements merely correlate with wall-clock time. Wall-clock
time is an obvious choice in many cases because it corresponds to what users
perceive and Criterion-rs measures it directly.</li>
<li>Iai-Callgrind can only be used on platforms supported by Valgrind. Notably,
this does not include Windows.</li>
<li>Iai-Callgrind needs additional binaries, <code>valgrind</code> and the
<code>iai-callgrind-runner</code>. The version of the runner needs to be in sync with the
<code>iai-callgrind</code> library. Criterion-rs is only a library and the installation
is usually simpler.</li>
</ul>
<p>Especially, due to the first point in the <code>Cons</code>, I think it is still required
to run wall-clock time benchmarks and use <code>Criterion-rs</code> in conjunction with
Iai-Callgrind. But in the CI and for performance regression checks, you
shouldn't use <code>Criterion-rs</code> or other wall-clock time based benchmarks at all.</p>
<div class="footnote-definition" id="note"><sup class="footnote-definition-label">1</sup>
<p><a href="https://sqlite.org/cpu.html#performance_measurement">https://sqlite.org/cpu.html#performance_measurement</a></p>
</div>
<div style="break-before: page; page-break-before: always;"></div><h1 id="comparison-of-iai-callgrind-with-iai"><a class="header" href="#comparison-of-iai-callgrind-with-iai">Comparison of Iai-Callgrind with Iai</a></h1>
<p>This is a comparison with <a href="https://github.com/bheisler/iai">Iai</a>, from which
Iai-Callgrind is forked over a year ago.</p>
<p>Iai-Callgrind Pros:</p>
<ul>
<li>
<p>Iai-Callgrind is actively maintained.</p>
</li>
<li>
<p>The benchmark api of Iai-Callgrind is simple, intuitive and allows for a much
more concise and clearer structure of benchmarks.</p>
</li>
<li>
<p>More stable metrics because the benchmark function is virtually encapsulated
by Callgrind and separates the benchmarked code from the surrounding code.</p>
</li>
<li>
<p>Iai-Callgrind excludes setup code from the metrics natively.</p>
</li>
<li>
<p>The Callgrind output files are much more focused on the benchmark function and
the function under test than the Cachegrind output files that Iai produces.
The calibration run of Iai only sanitized the visible summary output but not
the metrics in the output files themselves. So, the output of <code>cg_annotate</code>
was still cluttered by the initialization code, setup functions and metrics.</p>
</li>
<li>
<p>Changes to the library of Iai-Callgrind have almost never an influence on the
benchmark metrics, since the actual runner (<code>iai-callgrind-runner</code>) and thus
<code>99%</code> of the code needed to run the benchmarks is isolated from the
benchmarks by an independent binary. In contrast to the library of Iai which
is compiled together with the benchmarks.</p>
</li>
<li>
<p>Iai-Callgrind has functionality in place that provides a more constant
environment, like the <code>Sandbox</code> and clearing environment variables.</p>
</li>
<li>
<p>Supports running other Valgrind Tools, like DHAT, Massif etc.</p>
</li>
<li>
<p>Comparison of benchmark functions.</p>
</li>
<li>
<p>Iai-Callgrind can be configured to check for performance regressions.</p>
</li>
<li>
<p>A complete implementation of Valgrind Client Requests is available in
Iai-Callgrind itself.</p>
</li>
<li>
<p>Comparison of benchmarks to baselines instead of only to <code>.old</code> files.</p>
</li>
<li>
<p>Iai-Callgrind natively supports benchmarking binaries.</p>
</li>
<li>
<p>Iai-Callgrind can print machine-readable output in <code>.json</code> format.</p>
</li>
</ul>
<p>I don't see any downside in using Iai-Callgrind instead of Iai.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->
        <script src="js/dropdown.js"></script>

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
